[
  {
    "book_name": "custom-database.html",
    "abstract": "介绍如何自定义Hypothesis数据库，包括实现SQLiteExampleDatabase类及其save、fetch、delete方法，并说明move方法的默认行为及重写方式。",
    "content": "# Write a custom Hypothesis database\n\nTo define your own `ExampleDatabase` class, implement the `save()`, `fetch()`, and `delete()` methods.\n\nFor example, here’s a simple database class that uses sqlite as the backing data store:\n\n```python\nimport sqlite3\nfrom collections.abc import Iterable\nfrom hypothesis.database import ExampleDatabase\n\nclass SQLiteExampleDatabase(ExampleDatabase):\n    def __init__(self, db_path: str):\n        self.conn = sqlite3.connect(db_path)\n        self.conn.execute(\n            \"\"\"\n            CREATE TABLE examples (\n                key BLOB,\n                value BLOB,\n                UNIQUE (key, value)\n            )\n            \"\"\"\n        )\n\n    def save(self, key: bytes, value: bytes) -> None:\n        self.conn.execute(\n            \"INSERT OR IGNORE INTO examples VALUES (?, ?)\",\n            (key, value),\n        )\n\n    def fetch(self, key: bytes) -> Iterable[bytes]:\n        cursor = self.conn.execute(\"SELECT value FROM examples WHERE key = ?\", (key,))\n        yield from [value[0] for value in cursor.fetchall()]\n\n    def delete(self, key: bytes, value: bytes) -> None:\n        self.conn.execute(\n            \"DELETE FROM examples WHERE key = ? AND value = ?\",\n            (key, value),\n        )\n```\n\nDatabase classes are not required to implement `move()`. The default implementation of a move is a `delete()` of the value in the old key, followed by a `save()` of the value in the new key. You can override `move()` to override this behavior, if for instance the backing store offers a more efficient move implementation."
  },
  {
    "book_name": "custom-database.html",
    "abstract": "说明如何在自定义数据库类中支持变更监听，包括调用_broadcast_change()方法，并提及_start_listening()和_stop_listening()方法用于管理监听操作。",
    "content": "\n\n## Change listening\n\nTo support change listening in a database class, you should call `_broadcast_change()` whenever a value is saved, deleted, or moved in the backing database store. How you track this depends on the details of the database class. For instance, in `DirectoryBasedExampleDatabase`, Hypothesis installs a filesystem monitor via watchdog in order to broadcast change events.\n\nTwo useful related methods are `_start_listening()` and `_stop_listening()`, which a database class can override to know when to start or stop expensive listening operations. See documentation for details."
  },
  {
    "book_name": "custom-strategies.html",
    "abstract": "介绍如何编写自定义策略，包括编写辅助函数和使用@composite装饰器创建新策略。",
    "content": "Custom strategies\n=================\n\nThis page describes how to write a custom strategy, for when the built-in strategies don’t quite fit your needs.\n\nWriting helper functions\n------------------------\n\nSometimes you might find it useful to write helper functions, to more concisely express a common pattern for your project. For example, it’s much easier to write (and read!) `response=json()` than to have the whole implementation inline:\n\n```python\ndef json(*, finite_only=True):\n    \"\"\"Helper function to describe JSON objects, with optional inf and nan.\"\"\"\n    numbers = st.floats(allow_infinity=not finite_only, allow_nan=not finite_only)\n    return st.recursive(\n        st.none() | st.booleans() | st.integers() | numbers | st.text(),\n        extend=lambda xs: st.lists(xs) | st.dictionaries(st.text(), xs),\n    )\n```\n\nWriting your own strategy\n-------------------------\n\nIf a strategy in Hypothesis doesn’t match what you need, you can write your own strategy.\n\nFor instance, suppose we want to generate a list of floats which sum to 1. We might start implementing this by generating lists of integers between 0 and 1 with `lists(floats(0, 1))`. But now we’re a bit stuck, and can’t go any further with the standard strategies.\n\nOne way to define a new strategy is using the `@composite` decorator. `@composite` lets you define a new strategy that uses arbitrary Python code. For instance, to implement the above:\n\n```python\nfrom hypothesis import strategies as st\n\n@st.composite\ndef sums_to_one(draw):\n    l = draw(st.lists(st.floats(0, 1)))\n    return [f / sum(l) for f in l]\n```\n\n`@composite` passes a `draw` function to the decorated function as its first argument."
  },
  {
    "book_name": "custom-strategies.html",
    "abstract": "介绍Hypothesis库中@composite装饰器的基本用法，通过sums_to_one示例展示如何创建自定义策略，并演示其在属性测试中的应用。",
    "content": " `draw` is used to draw a random value from another strategy. We return from `sums_to_one` a value of the form we wanted to generate; in this case, a list that sums to one.\n\nLet’s see this new strategy in action:\n\n```python\nimport pytest\nfrom hypothesis import given, strategies as st\n\n@st.composite\ndef sums_to_one(draw):\n    lst = draw(st.lists(st.floats(0.001, 1), min_size=1))\n    return [f / sum(lst) for f in lst]\n\n@given(sums_to_one())\ndef test(lst):\n    # ignore floating point errors\n    assert sum(lst) == pytest.approx(1)\n```\n\n> **Note**\n>\n> Just like all other strategies, we called `sums_to_one` before passing it to `@given`. `@composite` should be thought of as turning its decorated function into a function which returns a strategy when called. This is actually the same as existing strategies in Hypothesis; `integers()` is really a function, which returns a strategy for integers when called."
  },
  {
    "book_name": "custom-strategies.html",
    "abstract": "介绍如何在使用 @composite 装饰器的函数中添加普通参数和仅关键字参数，并通过 sums_to_n 示例展示其用法。",
    "content": "\n\nCombining `@composite` with parameters\n--------------------------------------\n\nYou can add parameters to functions decorated with `@composite`, including keyword-only arguments. These work as you would normally expect.\n\nFor instance, suppose we wanted to generalize our `sums_to_one` function to `sums_to_n`. We can add a parameter `n`:\n\n```python\nimport pytest\nfrom hypothesis import assume, given, strategies as st\n\n@st.composite\ndef sums_to_n(draw, n=1):  # <-- changed\n    lst = draw(st.lists(st.floats(0, 1), min_size=1))\n    assume(sum(lst) > 0)\n    return [f / sum(lst) * n for f in lst]  # <-- changed\n\n@given(sums_to_n(10))\ndef test(lst):\n    assert sum(lst) == pytest.approx(10)\n```"
  },
  {
    "book_name": "custom-strategies.html",
    "abstract": "展示了如何将 `n` 设为仅限关键字参数，并介绍了使用 `@composite` 装饰器生成相互依赖的值（如有序整数对）的方法。",
    "content": "\n\nAnd we could just as easily have made `n` a keyword-only argument instead:\n\n```python\nimport pytest\nfrom hypothesis import assume, given, strategies as st\n\n@st.composite\ndef sums_to_n(draw, *, n=1):  # <-- changed\n    lst = draw(st.lists(st.floats(0, 1), min_size=1))\n    assume(sum(lst) > 0)\n    return [f / sum(lst) * n for f in lst]\n\n@given(sums_to_n(n=10))  # <-- changed\ndef test(lst):\n    assert sum(lst) == pytest.approx(10)\n```\n\nDependent generation with `@composite`\n--------------------------------------\n\nAnother scenario where `@composite` is useful is when generating a value that depends on a value from another strategy. For instance, suppose we wanted to generate two integers `n1` and `n2` with `n1 <= n2`. We can do this using `@composite`:\n\n```python\n@st.composite\ndef ordered_pairs(draw):\n    n1 = draw(st.integers())\n    n2 = draw(st.integers(min_value=n1))\n    return (n1, n2)\n\n@given(ordered_pairs())\ndef test_pairs_are_ordered(pair):\n    n1, n2 = pair\n    assert n1 <= n2\n```\n\n> **Note**\n>\n> We could also have written this particular strategy as `st.tuples(st.integers(), st.integers()).map("
  },
  {
    "book_name": "custom-strategies.html",
    "abstract": "介绍了在测试中混合数据生成与测试代码的方法，对比了`@composite`和`data()`的使用场景，并指出了`data()`的缺点。",
    "content": "sorted)` (see Adapting strategies). Some prefer this inline approach, while others prefer defining well-named helper functions with `@composite`. Our suggestion is simply that you prioritize ease of understanding when choosing which to use.\n\nMixing data generation and test code\n------------------------------------\n\nWhen using `@composite`, you have to finish generating the entire input before running your test. But maybe you don’t want to generate all of the input until you’re sure some initial test assertions have passed. Or maybe you have some complicated control flow which makes it necessary to generate something in the middle of the test.\n\n`data()` lets you do this. It’s similar to `@composite`, except it lets you mix test code and generation code.\n\n> **Note**\n>\n> The downside of this power is that `data()` is incompatible with `@example`, and that Hypothesis cannot print a nice representation of values generated from `data()` when reporting failing examples, because the draws are spread out. Where possible, prefer `@composite` to `data()`.\n\nFor instance, here’s how we would write our earlier `@composite` example using `data()`:"
  },
  {
    "book_name": "detect-hypothesis-tests.html",
    "abstract": "介绍了两种动态检测 Hypothesis 测试函数的方法：一是使用 `is_hypothesis_test()` 工具函数，该方法对普通测试和基于状态机的测试均有效；二是利用 pytest 插件自动添加的 `hypothesis` 标记进行检测。",
    "content": "# Detect Hypothesis tests\n\nHow to dynamically determine whether a test function has been defined with Hypothesis.\n\n## Via `is_hypothesis_test`\n\nThe most straightforward way is to use `is_hypothesis_test()`:\n\n```python\nfrom hypothesis import is_hypothesis_test\n\n@given(st.integers())\ndef f(n): ...\n\nassert is_hypothesis_test(f)\n```\n\nThis works for stateful tests as well:\n\n```python\nfrom hypothesis import is_hypothesis_test\nfrom hypothesis.stateful import RuleBasedStateMachine\n\nclass MyStateMachine(RuleBasedStateMachine): ...\n\nassert is_hypothesis_test(MyStateMachine.TestCase().runTest)\n```\n\n## Via pytest\n\nIf you’re working with pytest, the Hypothesis pytest plugin automatically adds the `@pytest.mark.hypothesis` mark to all Hypothesis tests. You can use `node.get_closest_marker(\"hypothesis\")` or similar methods to detect the existence of this mark."
  },
  {
    "book_name": "example-count.html",
    "abstract": "Hypothesis测试运行次数通常为max_examples次，但存在三种例外情况：搜索空间耗尽、因assume()/filter()导致重试、或发现失败用例。",
    "content": " The short answer is “exactly max_examples times”, with the following exceptions:\n\n- Less than max_examples times, if Hypothesis exhausts the search space early.\n- More than max_examples times, if Hypothesis retries some examples because either:\n- Either less or more than max_examples times, if Hypothesis finds a failing example.\n\nRead on for details."
  },
  {
    "book_name": "example-count.html",
    "abstract": "介绍了 Hypothesis 在搜索空间耗尽时的行为，即当没有更多示例可尝试时，会提前停止生成。并通过整数范围策略的示例说明了这一点。",
    "content": "\n\n## Search space exhaustion\n\nIf Hypothesis detects that there are no more examples left to try, it may stop generating examples before it hits max_examples. For example:\n\n```python\nfrom hypothesis import given, strategies as st\n\ncalls = 0\n\n@given(st.integers(0, 19))\ndef test_function(n):\n    global calls\n    calls += 1\n\ntest_function()\nassert calls == 20\n```\n\nThis runs `test_function` 20 times, not 100, since there are only 20 unique integers to try.\n\nThe search space tracking in Hypothesis is good, but not perfect. We treat this more as a bonus than something to strive for."
  },
  {
    "book_name": "example-count.html",
    "abstract": "(part 1/2) 介绍了 `assume()` 和 `.filter()` 的工作方式，它们在条件不满足时会重试生成示例且不计入 `max_examples` 限制，并比较了两者的效率差异。",
    "content": "\n\n## `assume()` and `.filter()`\n\nIf an example fails to satisfy an `assume()` or `.filter()` condition, Hypothesis will retry generating that example and will not count it towards the `max_examples` limit. For instance:\n\n```python\nfrom hypothesis import assume, given, strategies as st\n\n@given(st.integers())\ndef test_function(n):\n    assume(n % 2 == 0)\n```\n\nwill run roughly 200 times, since half of the examples are discarded from the `assume()`.\n\nNote that while failing an `assume()` triggers an immediate retry of the entire example, Hypothesis will try several times in the same example to satisfy a `.filter()` condition. This makes expressing the same condition using `.filter()` more efficient than `assume()`."
  },
  {
    "book_name": "example-count.html",
    "abstract": "(part 2/2) 说明了内置策略可能隐式使用 `assume()` 或 `.filter()`，并介绍了因示例过大或测试失败而触发的重试和额外执行行为，包括健康检查、收缩和解释阶段。",
    "content": "\n\nAlso note that even if your code does not explicitly use `assume()` or `.filter()`, a builtin strategy may still use them and cause retries. We try to directly satisfy conditions where possible instead of relying on rejection sampling, so this should be relatively uncommon.\n\n## Examples which are too large\n\nFor performance reasons, Hypothesis places an internal limit on the size of a single example. If an example exceeds this size limit, we will retry generating it and will not count it towards the `max_examples` limit. (And if we see too many of these large examples, we will raise `HealthCheck.data_too_large`, unless suppressed with `settings.suppress_health_check`).\n\nThe specific value of this size limit is an undocumented implementation detail. The majority of Hypothesis tests do not come close to hitting it.\n\n## Failing examples\n\nIf Hypothesis finds a failing example, it stops generation early, and may call the test function additional times during the `Phase.shrink` and `Phase.explain` phases. Sometimes, Hypothesis determines that the initial failing example was already as simple as possible, in which case `Phase.shrink` will not result in additional test executions (but `Phase.explain` might).\n\nRegardless of whether Hypothesis runs the test during the shrinking and explain phases, it will always run the minimal failing example one additional time to check for flakiness. For instance, the following trivial test runs with `n=0` twice, even though it only uses the `Phase.generate` phase:\n\nThe first execution finds the initial failure with `n=0`, and the second execution replays `n=0` to ensure the failure is not flaky."
  },
  {
    "book_name": "external-fuzzers.html",
    "abstract": "介绍了将Hypothesis与外部模糊测试工具（如python-afl或atheris）结合使用的场景和动机，并引出了`fuzz_one_input()`方法作为核心接口。",
    "content": "Use Hypothesis with an external fuzzer\n=====================================\n\nSometimes you might want to point a traditional fuzzer like python-afl or Google’s atheris at your code, to get coverage-guided exploration of native C extensions. The associated tooling is often much less mature than property-based testing libraries though, so you might want to use Hypothesis strategies to describe your input data, and our world-class shrinking and observability tools to wrangle the results. That’s exactly what this how-to guide is about!"
  },
  {
    "book_name": "external-fuzzers.html",
    "abstract": "介绍 Hypothesis 的 fuzz_one_input() 方法，用于将 Hypothesis 测试作为传统模糊测试目标，并说明其绕过标准测试生命周期的特性。",
    "content": "\n\nNote\n----\n\nIf you already have Hypothesis tests and want to fuzz them, or are targeting pure Python code, we strongly recommend the purpose-built HypoFuzz. This page is about writing traditional ‘fuzz harnesses’ with an external fuzzer, using parts of Hypothesis.\n\nIn order to support this workflow, Hypothesis exposes the `fuzz_one_input()` method. `fuzz_one_input()` takes a bytestring, parses it into a test case, and executes the corresponding test once. This means you can treat each of your Hypothesis tests as a traditional fuzz target, by pointing the fuzzer at `fuzz_one_input()`.\n\nFor example:\n\n```python\nfrom hypothesis import given, strategies as st\n\n@given(st.integers())\ndef test_ints(n):\n    pass\n\n# this parses the bytestring into a test case using st.integers(),\n# and then executes `test_ints` once.\ntest_ints.hypothesis.fuzz_one_input(b\"\\x00\" * 50)\n```\n\nNote that `fuzz_one_input()` bypasses the standard test lifecycle. In a standard test run, Hypothesis is responsible for managing the lifecycle of a test, for example by moving between each `Phase`. In contrast, `fuzz_one_input()` executes one test case, independent of this lifecycle.\n\nSee the documentation of `fuzz_one_input()` for details of how it interacts with other features of Hypothesis, such as `@settings`."
  },
  {
    "book_name": "external-fuzzers.html",
    "abstract": "展示了如何结合Atheris和Hypothesis进行模糊测试，通过Hypothesis生成有效的JSON对象并用`json.dumps`进行测试。",
    "content": "\n\nWorked example: using Atheris\n-----------------------------\n\nHere is an example that uses `fuzz_one_input()` with the Atheris coverage-guided fuzzer (which is built on top of libFuzzer):\n\n```python\nimport json\nimport sys\nimport atheris\nfrom hypothesis import given, strategies as st\n\n@given(\n    st.recursive(\n        st.none() | st.booleans() | st.integers() | st.floats() | st.text(),\n        lambda j: st.lists(j) | st.dictionaries(st.text(), j),\n    )\n)\ndef test_json_dumps_valid_json(value):\n    json.dumps(value)\n\natheris.Setup(sys.argv, test_json_dumps_valid_json.hypothesis.fuzz_one_input)\natheris.Fuzz()"
  },
  {
    "book_name": "external-fuzzers.html",
    "abstract": "指出仅用Atheris原生接口生成有效JSON更困难，并建议使用`atheris.instrument_all`或`atheris.instrument_imports`来增加覆盖率检测。",
    "content": "\n```\n\nGenerating valid JSON objects based only on Atheris’ `FuzzDataProvider` interface would be considerably more difficult.\n\nYou may also want to use `atheris.instrument_all` or `atheris.instrument_imports` in order to add coverage instrumentation to Atheris. See the Atheris documentation for full details."
  },
  {
    "book_name": "flaky.html",
    "abstract": "介绍了“Flaky failures”（不稳定失败）的概念，即测试在重试时可能从失败变为通过，这种非确定性的测试被称为 flaky test。",
    "content": "Flaky failures\n\nHave you ever had a test fail, and then you re-run it, only for the test to magically pass? That is a flaky test. A flaky test is one which might behave differently when called again. You can think of it as a test which is not deterministic."
  },
  {
    "book_name": "flaky.html",
    "abstract": "解释了任何测试都可能存在不稳定性，而 Hypothesis 由于会多次运行测试，因此特别容易暴露出这种不稳定行为。",
    "content": "\n\nAny test can be flaky, but because Hypothesis runs your test many times, Hypothesis tests are particularly likely to uncover flaky behavior."
  },
  {
    "book_name": "integrations.html",
    "abstract": "Hypothesis Ghostwriter 工具介绍 (part 1)",
    "content": "Integrations Reference\n======================\n\nReference for Hypothesis features with a defined interface, but no code API.\n\nGhostwriter\n-----------\n\nWriting tests with Hypothesis frees you from the tedium of deciding on and writing out specific inputs to test. Now, the `hypothesis.extra.ghostwriter` module can write your test functions for you too!"
  },
  {
    "book_name": "integrations.html",
    "abstract": "Introduction to property-based testing with ghostwritten tests and command-line interface examples.",
    "content": "\n\nThe idea is to provide an easy way to start property-based testing, and a seamless transition to more complex test code—because ghostwritten tests are source code that you could have written for yourself.\n\nSo just pick a function you’d like tested, and feed it to one of the functions below. They follow imports, use but do not require type annotations, and generally do their best to write you a useful test. You can also use our command-line interface:\n\n```console\n$ hypothesis write --help\nUsage: hypothesis write [OPTIONS] FUNC...\n\n`hypothesis write` writes property-based tests for you!\n\nType annotations are helpful but not required for our advanced introspection and templating logic. Try running the examples below to see how it works:\n\nhypothesis write gzip\nhypothesis write numpy.matmul\nhypothesis write pandas.from_dummies\nhypothesis write re.compile --except re.error\nhypothesis write --equivalent ast.literal_eval eval\nhypothesis write --roundtrip json.dumps json.loads\nhypothesis write --style=unittest --idempotent sorted\nhypothesis write --binary-op operator.add\n\nOptions:\n  --roundtrip              start by testing write/read or encode/decode!\n  --equivalent             very useful when optimising or refactoring code\n  --errors-equivalent      --equivalent, but also allows consistent errors\n  --idempotent             check that f(x) == f(f(x))\n  --binary-op              associativity, commutativity, identity element\n  --style [pytest|unittest] pytest-style function, or unittest-style method?\n  -e, --except OBJ_NAME    dotted name of exception(s) to ignore\n  --annotate / --no-annotate force ghostwritten tests to be type-"
  },
  {
    "book_name": "internals.html",
    "abstract": "Hypothesis internals introduction and warning",
    "content": "Hypothesis internals\n\nWarning\n\nThis page documents internal Hypothesis interfaces. Some are fairly stable, while others are still experimental. In either case, they are not subject to our standard deprecation policy, and we might make breaking changes in minor or patch releases.\n\nThis page is intended for people building tools, libraries, or research on top of Hypothesis. If that includes you, ple"
  },
  {
    "book_name": "internals.html",
    "abstract": "介绍 Hypothesis 的 PrimitiveProvider 接口及其 lifetime 属性，说明其在测试函数或测试用例级别的生命周期管理。",
    "content": "ase get in touch! We’d love to hear what you’re doing, or explore more stable ways to support your use-case.\n\nAlternative backends\n\nSee also\n\nSee also the user-facing Alternative backends for Hypothesis documentation.\n\n- class hypothesis.internal.conjecture.providers.PrimitiveProvider(conjecturedata, /)[source]\n\nPrimitiveProvider is the implementation interface of a Hypothesis backend. A PrimitiveProvider is required to implement the following five draw_* methods:\n\nEach strategy in Hypothesis generates values by drawing a series of choices from these five methods. By overriding them, a PrimitiveProvider can control the distribution of inputs generated by Hypothesis.\n\nFor example, hypothesis-crosshair implements a PrimitiveProvider which uses an SMT solver to generate inputs that uncover new branches.\n\nOnce you implement a PrimitiveProvider, you can make it available for use through AVAILABLE_PROVIDERS.\n\n- lifetime = 'test_function'\n\nThe lifetime of a PrimitiveProvider instance. Either test_function or test_case.\n\nIf test_function (the default), a single provider instance will be instantiated and used for the entirety of each test function (i.e., roughly one provider per @given annotation). This can be useful for tracking state over the entirety of a test function.\n\nIf test_case, a new provider instance will be instantiated and used for each input Hypothesis generates.\n\nThe conjecturedata argument to PrimitiveProvider.__init__ will be None for a lifetime of test_function, and an instance of ConjectureData for a lifetime of test_case.\n\nThird-party providers likely want to set"
  },
  {
    "book_name": "internals.html",
    "abstract": "描述 PrimitiveProvider 的 avoid_realization 和 add_observability_callback 属性，以及 draw_boolean 和 draw_integer 方法的参数与行为。",
    "content": " a lifetime of test_function.\n\n- avoid_realization = False\n\nSolver-based backends such as hypothesis-crosshair use symbolic values which record operations performed on them in order to discover new paths. If avoid_realization is set to True, hypothesis will avoid interacting with symbolic choices returned by the provider in any way that would force the solver to narrow the range of possible values for that symbolic.\n\nSetting this to True disables some hypothesis features and optimizations. Only set this to True if it is necessary for your backend.\n\n- add_observability_callback = False\n\nwill never be called by Hypothesis.\n\nThe opt-in behavior of observability is because enabling observability might increase runtime or memory usage.\n\n- abstract draw_boolean(p=0.5)[source]\n\nDraw a boolean choice.\n\nParameters:\n  p (float) – The probability of returning True. Between 0 and 1 inclusive. Except for 0 and 1, the value of p is a hint provided by Hypothesis, and may be ignored by the backend. If 0, the provider must return False. If 1, the provider must return True.\n\n- abstract draw_integer(min_value=None, max_value=None, *, weights=None, shrink_towards=0)[source]\n\nDraw an integer choice.\n\nParameters:\n  min_value (int | None) – (Inclusive) lower bound on the integer value. If None, there is no lower bound.\n  max_value (int | None) – (Inclusive) upper bound on the integer value. If None, there is no upper bound.\n  weights (dict[int, float] | None) – Maps keys in the range [min_value, max_value] to the probability of returning that key.\n  shrink_towards (int) – The integer to shrink towards. This is not used during generation and can be ignored by backends."
  },
  {
    "book_name": "internals.html",
    "abstract": "描述 PrimitiveProvider 的 draw_float 和 draw_string 方法的参数，以及 per_test_case_context_manager 和 realize 方法的作用。",
    "content": "\n\n- abstract draw_float(*, min_value=-inf, max_value=inf, allow_nan=True, smallest_nonzero_magnitude)[source]\n\nDraw a float choice.\n\nParameters:\n  min_value (float) – (Inclusive) lower bound on the float value.\n  max_value (float) – (Inclusive) upper bound on the float value.\n  allow_nan (bool) – If False, it is invalid to return math.nan.\n  smallest_nonzero_magnitude (float) – The smallest allowed nonzero magnitude. draw_float should not return a float f if abs(f) < smallest_nonzero_magnitude.\n\n- abstract draw_string(intervals, *, min_size=0, max_size=10000000000)[source]\n\nDraw a string choice.\n\nParameters:\n  intervals (IntervalSet) – The set of codepoints to sample from.\n  min_size (int) – (Inclusive) lower bound on the string length.\n  max_size (int) – (Inclusive) upper bound on the string length.\n\n- per_test_case_context_manager()[source]\n\nReturns a context manager which will be entered each time Hypothesis starts generating and executing one test case, and exited when that test case finishes generating and executing, including if any exception is thrown.\n\nIn the lifecycle of a Hypothesis test, this is called before generating strategy values for each test case. This is just before any custom executor is called.\n\nEven if not returning a custom context manager, PrimitiveProvider subclasses are welcome to override this method to know when Hypothesis starts and ends the execution of a single test case.\n\n- realize(value, *, for_failure=False)[source]\n\nCalled whenever hypothesis requires a concrete (non-symbolic) value from a potentially symbolic value. Hypothesis will not check that value is symbolic before calling realize, so you should handle the case where value is non-symbolic.\n\nThe returned value should be non-symbolic. If you cannot provide a value, raise BackendCannotProceed with a value of \"discard_test_case\"."
  },
  {
    "book_name": "internals.html",
    "abstract": "描述 PrimitiveProvider 的 replay_choices、observe_test_case、observe_information_messages 和 on_observation 方法的功能，涉及失败用例处理、观测数据收集与信息消息传递。",
    "content": "\n\nIf for_failure is True, the value is associated with a failing example. In this case, the backend should spend substantially more effort when attempting to realize the value, since it is important to avoid discarding failing examples. Backends may still raise BackendCannotProceed when for_failure is True, if realization is truly impossible or if realization takes significantly longer than expected (say, 5 minutes).\n\n- replay_choices(choices)[source]\n\nCalled when Hypothesis has discovered a choice sequence which the provider may wish to enqueue to replay under its own instrumentation when we next ask to generate a test case, rather than generating one from scratch.\n\nThis is used to e.g. warm-start hypothesis-crosshair with a corpus of high-code-coverage inputs discovered by HypoFuzz.\n\n- observe_test_case()[source]\n\nCalled at the end of the test case when observability is enabled.\n\nThe return value should be a non-symbolic json-encodable dictionary, and will be included in observations as observation[\"metadata\"][\"backend\"].\n\n- observe_information_messages(*, lifetime)[source]\n\nCalled at the end of each test case and again at end of the test function.\n\nReturn an iterable of {type: info/alert/error, title: str, content: str | dict} dictionaries to be delivered as individual information messages. Hypothesis adds the run_start timestamp and property name for you.\n\n- on_observation(observation)[source]\n\nCalled at the end of each test case which uses this provider, with the same observation[\"type\"] == \"test_case\" observation that is passed to other callbacks added via add_observability_callback. This method is not called with observation[\"type\"] in {\"info\""
  },
  {
    "book_name": "introduction.html",
    "abstract": "Hypothesis简介、安装方法及一个简单的整数测试示例。",
    "content": "Introduction to Hypothesis\n===========================\n\nThis page introduces two fundamental parts of Hypothesis (`@given` and strategies) and shows how to test a selection sort implementation using Hypothesis.\n\nInstall Hypothesis\n------------------\n\nFirst, let’s install Hypothesis:\n\n```bash\npip install hypothesis\n```\n\nDefining a simple test\n----------------------\n\nHypothesis tests are defined using two things: `@given` and a strategy, which is passed to `@given`. Here’s a simple example:\n\n```python\nfrom hypothesis import given, strategies as st\n\n@given(st.integers())\ndef test_is_integer(n):\n    assert isinstance(n, int)\n```\n\nAdding the `@given` decorator turns this function into a Hypothesis test. Passing `integers()` to `@given` says that Hypothesis should generate random integers for the argument `n` when testing."
  },
  {
    "book_name": "introduction.html",
    "abstract": "介绍了如何使用 Hypothesis 库进行属性测试，包括测试一个整数检查函数和一个选择排序算法。通过 `@given` 装饰器和策略（如 `st.integers()` 和 `st.lists()`），Hypothesis 能自动生成测试用例。",
    "content": "\n\nWe can run this test by calling it:\n\n```python\nfrom hypothesis import given, strategies as st\n\n@given(st.integers())\ndef test_is_integer(n):\n    print(f\"called with {n}\")\n    assert isinstance(n, int)\n\ntest_is_integer()\n```\n\nNote that we don’t pass anything for `n`; Hypothesis handles generating that value for us. The resulting output looks like this:\n\n```\ncalled with 0\ncalled with -18588\ncalled with -672780074\ncalled with 32616\n...\n```\n\nTesting a sorting algorithm\n--------------------------\n\nSuppose we have implemented a simple selection sort and want to make sure it’s correct. We can write the following test by combining the `integers()` and `lists()` strategies:\n\n```python\n@given(st.lists(st.integers()))\ndef test_sort_correct(lst):\n    print(f\"called with {lst}\")\n    assert selection_sort(lst.copy()) == sorted(lst)\n\ntest_sort_correct()\n```\n\nWhen running `test_sort_correct`, Hypothesis uses the `lists(integers())` strategy to generate random lists of integers. Feel free to run `python example.py` to get an idea of the kinds of lists Hypothesis generates (and to convince yourself that this test passes)."
  },
  {
    "book_name": "introduction.html",
    "abstract": "介绍了如何使用 Hypothesis 库的策略组合（`|` 操作符）来生成包含整数或浮点数的列表以测试排序函数，并指出由于 `nan` 值导致排序行为未定义，因此需要通过 `floats(allow_nan=False)` 来排除 `nan`。",
    "content": "\n\nAdding floats to our test\n-------------------------\n\nThis test is a good start. But `selection_sort` should be able to sort lists with floats, too. If we wanted to generate lists of either integers or floats, we can change our strategy:\n\n```python\n# changes to example.py\n@given(st.lists(st.integers() | st.floats()))\ndef test_sort_correct(lst):\n    pass\n```\n\nThe pipe operator `|` takes two strategies and returns a new strategy which generates values from either of its strategies. So the strategy `integers() | floats()` can generate either an integer or a float.\n\n> **Note**  \n> `strategy1 | strategy2` is equivalent to `st.one_of(strategy1, strategy2)`.\n\nPreventing `floats()` from generating `nan`\n------------------------------------------\n\nEven though `test_sort_correct` passed when we used lists of integers, it actually fails now that we’ve added floats! If you run `python example.py`, you’ll likely (but not always; this is random testing, after all) find that Hypothesis reports a counterexample to `test_sort_correct`. For me, that counterexample is `[1.0, nan, 0]`. It might be different for you.\n\nThe issue is that sorting in the presence of `nan` is not well defined. As a result, we may decide that we don’t want to generate them while testing. We can pass `floats(allow_nan=False)` to tell Hypothesis not to generate `nan`:\n\n```python\n# changes to example.py\n@given(st.lists(st.integers() | st.floats(allow_nan=False)))\ndef test_sort_correct(lst):\n    pass\n```\n\nAnd now this test passes without issues."
  },
  {
    "book_name": "introduction.html",
    "abstract": "说明了如何使用 Hypothesis 为测试函数提供多个参数，包括位置参数和关键字参数，并介绍了运行 Hypothesis 测试的几种方式。",
    "content": "\n\n> **Note**  \n> You can use the `.example()` method to get an idea of the kinds of things a strategy will generate:\n>\n> ```python\n> >>> st.lists(st.integers() | st.floats(allow_nan=False)).example()\n> [-5.969063e-08, 15283673678, 18717, -inf]\n> ```\n>\n> Note that `.example()` is intended for interactive use only (i.e., in a REPL). It is not intended to be used inside tests.\n\nTests with multiple arguments\n-----------------------------\n\nIf we wanted to pass multiple arguments to a test, we can do this by passing multiple strategies to `@given`:\n\n```python\nfrom hypothesis import given, strategies as st\n\n@given(st.integers(), st.lists(st.floats()))\ndef test_multiple_arguments(n, lst):\n    assert isinstance(n, int)\n    assert isinstance(lst, list)\n    for f in lst:\n        assert isinstance(f, float)\n```\n\nKeyword arguments\n-----------------\n\nWe can also pass strategies using keyword arguments:\n\n```python\n@given(lst=st.lists(st.floats()), n=st.integers())  # <-- changed\ndef test_multiple_arguments(n, lst):\n    pass\n```\n\nNote that even though we changed the order the parameters to `@given` appear, we also explicitly told it which parameters to pass to by using keyword arguments, so the meaning of the test hasn’t changed.\n\nIn general, you can think of positional and keyword arguments to `@given` as being forwarded to the test arguments.\n\nRunning Hypothesis tests\n------------------------\n\nThere are a few ways to run a Hypothesis test:\n\n- Explicitly call it, like `test_is_integer()`, as we’ve seen. Hypothesis tests are just normal functions, except `@given` handles generating and passing values for the function arguments.\n- Let a test runner such as `pytest` pick up on it (as long as the function name star"
  },
  {
    "book_name": "introduction.html",
    "abstract": "讨论了何时使用 Hypothesis 和基于属性的测试，建议寻找往返属性、替换参数化测试，并列举了其他类型的可测试属性，如优化实现的等价性、金融交易的平衡性等。",
    "content": "ts with `test_`).\n\nConcretely, when running a Hypothesis test, Hypothesis will:\n\n- Generate 100 random inputs,\n- Run the body of the function for each input, and\n- Report any exceptions that get raised.\n\n> **Note**  \n> The number of examples can be controlled with the `max_examples` setting. The default is 100.\n\nWhen to use Hypothesis and property-based testing\n------------------------------------------------\n\nProperty-based testing is a powerful addition to unit testing. It is not always a replacement.\n\nIf you’re having trouble coming up with a property in your code to test, we recommend trying the following:\n\n- Look for round-trip properties: encode/decode, serialize/deserialize, etc. These property-based tests tend to be both powerful and easy to write.\n- Look for `@pytest.mark.parametrize` in your existing tests. This is sometimes a good hint you can replace the parametrization with a strategy. For instance, `@pytest.mark.parametrize(\"n\", range(0, 100))` could be replaced by `@given(st.integers(0, 100 - 1))`.\n- Simply call your code with random inputs (of the correct shape) from Hypothesis! You might be surprised how often this finds crashes. This can be especially valuable for projects with a single entrypoint interface to a lot of underlying code.\n\nOther examples of properties include:\n\n- An optimized implementation is equivalent to a slower, but clearly correct, implementation.\n- A sequence of transactions in a financial system always “balances”; money never gets lost.\n- The derivative of a polynomial of order `n` has order `n - 1`.\n- A type-checker, linter, formatter, or compiler does not crash when called on syntactically valid code."
  },
  {
    "book_name": "quickstart.html",
    "abstract": "Hypothesis快速入门指南的第一部分，介绍了安装方法、编写第一个测试用例以及`@given`装饰器的基本用法。",
    "content": "# Quickstart\n\nThis is a lightning introduction to the most important features of Hypothesis; enough to get you started writing tests. The tutorial introduces these features (and more) in greater detail.\n\n## Install Hypothesis\n\n```bash\npip install hypothesis\n```\n\n## Write your first test\n\nCreate a new file called `example.py`, containing a simple test:\n\n```python\n# contents of example.py\nfrom hypothesis import given, strategies as st\n\n@given(st.integers())\ndef test_integers(n):\n    print(f\"called with {n}\")\n    assert isinstance(n, int)\n\ntest_integers()\n```\n\n`@given` is the standard entrypoint to Hypothesis. It takes a strategy, which describes the type of inputs you want the decorated function to accept. When we call `test_integers`, Hypothesis will generate random integers (because we used the `integers()` strategy) and pass them as `n`. Let’s see that in action now by running `python example.py`:\n\n```\ncalled with 0\ncalled with -18588\ncalled with -672780074\ncalled with 32616\n...\n```\n\nWe just called `test_integers()`, without passing a value for `n`, because Hypothesis generates random values of `n` for us."
  },
  {
    "book_name": "quickstart.html",
    "abstract": "介绍了Hypothesis测试的基本概念，包括其默认生成100个随机输入、可被pytest或unittest识别为常规Python函数，并通过一个会失败的整数测试示例展示了其使用方法和错误输出。",
    "content": "\n\n> **Note**  \n> By default, Hypothesis generates 100 random inputs. You can control this with the `max_examples` setting.\n\n## Running in a test suite\n\nA Hypothesis test is still a regular Python function, which means pytest or unittest will pick it up and run it in all the normal ways.\n\n```python\n# contents of example.py\nfrom hypothesis import given, strategies as st\n\n@given(st.integers(0, 200))\ndef test_integers(n):\n    assert n < 50\n```\n\nThis test will clearly fail, which can be confirmed by running `pytest example.py`:\n\n```\n$ pytest example.py\n...\n@given(st.integers())\ndef test_integers(n):\n>   assert n < 50\nE   assert 50 < 50\nE   Falsifying example: test_integers(\nE       n=50,\nE   )"
  },
  {
    "book_name": "quickstart.html",
    "abstract": "说明了`@given`装饰器可以接受多个参数，并给出了一个同时使用整数和文本策略的测试函数示例。",
    "content": "\n```\n\n## Arguments to `@given`\n\nYou can pass multiple arguments to `@given`:\n\n```python\n@given(st.integers(), st.text())\ndef test_integers(n, s):\n    assert isinstance(n, int)"
  },
  {
    "book_name": "replaying-failures.html",
    "abstract": "介绍了 Hypothesis 测试中重放失败用例的重要性及主要方法，包括自动保存到本地数据库和手动重放。",
    "content": "Replaying failed tests\n======================\n\nReplaying failures found by your Hypothesis tests is almost as important as finding failures in the first place. Hypothesis therefore contains several ways to replay failures: they are automatically saved to (and replayed from) a local `ExampleDatabase`, and can be manually replayed via `@example` or `@reproduce_failure`."
  },
  {
    "book_name": "replaying-failures.html",
    "abstract": "文章剩余部分仅包含一个标题“The Hypothesis database”。",
    "content": "\n\nThe Hypothesis database\n----"
  },
  {
    "book_name": "settings.html",
    "abstract": "介绍了如何使用 @settings 装饰器来配置单个 Hypothesis 测试的行为，包括生成示例数量、重放失败示例和详细级别等，并说明了如何通过设置配置整个测试套件。",
    "content": "Configuring test settings\n=========================\n\nThis page discusses how to configure the behavior of a single Hypothesis test, or of an entire test suite.\n\nConfiguring a single test\n-------------------------\n\nHypothesis lets you configure the default behavior of a test using the `@settings` decorator. You can use settings to configure how many examples Hypothesis generates, how Hypothesis replays failing examples, and the verbosity level of the test, among others.\n\nUsing `@settings` on a single test looks like this:\n\n```python\nfrom hypothesis import given, settings, strategies as st\n\n@given(st.integers())\n@settings(max_examples=200)\ndef runs_200_times_instead_of_100(n):\n    pass\n```\n\nYou can put `@settings` either before or after `@given`. Both are equivalent.\n\nChanging the number of examples\n-------------------------------\n\nIf you have a test which is very expensive or very cheap to run, you can change the number of examples (inputs) Hypothesis generates with the `max_examples` setting:\n\nThe default is 100 examples.\n\n> **Note**\n>\n> See *How many times will Hypothesis run my test?* for details on how `max_examples` interacts with other parts of Hypothesis.\n\nOther settings options\n----------------------\n\nHere are a few of the more commonly used setting values:\n\n- `settings.phases` controls which phases of Hypothesis run, like replaying from the database or generating new inputs.\n- `settings.database` controls how and if Hypothesis replays failing examples.\n- `settings.verbosity` can print debug information.\n- `settings.derandomize` makes Hypothesis deterministic. (*Two kinds of testing* discusses when and why you might want that).\n\n> **Note**\n>\n> See the *settings reference* for a full list of possible settings.\n\nChanging settings across your test suite\n----------------------------------------\n\nIn addition to configuring individual test functions with `@settings`, you can configure test behavior across your test suite using a settings profile. This might be usef"
  },
  {
    "book_name": "settings.html",
    "abstract": "Hypothesis库的设置配置文件（settings profiles）功能介绍，包括如何注册、加载配置文件，以及通过环境变量动态选择配置。",
    "content": "ul for creating a development settings profile which runs fewer examples, or a settings profile in CI which connects to a separate database.\n\nTo create a settings profile, use `register_profile()`:\n\n```python\nfrom hypothesis import HealthCheck, settings\n\nsettings.register_profile(\"fast\", max_examples=10)\n```\n\nYou can place this code in any file which gets loaded before your tests get run. This includes an `__init__.py` file in the test directory or any of the test files themselves. If using pytest, the standard location to place this code is in a `conftest.py` file (though an `__init__.py` or test file will also work).\n\nNote that registering a new profile will not affect tests until it is loaded with `load_profile()`:\n\n```python\nfrom hypothesis import HealthCheck, settings\n\nsettings.register_profile(\"fast\", max_examples=10)\n# any tests executed before loading this profile will still use the\n# default active profile of 100 examples.\nsettings.load_profile(\"fast\")\n# any tests executed after this point will use the active fast\n# profile of 10 examples.\n```\n\nThere is no limit to the number of settings profiles you can create. Hypothesis creates a profile called `\"default\"`, which is active by default. You can also explicitly make it active at any time using `settings.load_profile(\"default\")`, if for instance you wanted to revert a custom profile you had previously loaded.\n\nLoading profiles from environment variables\n-------------------------------------------\n\nUsing an environment variable to load a settings profile is a useful trick for choosing a settings profile depending on the environment:\n\n```python\n>>> import os\n>>> from hypothesis import settings, Verbosity\n>>> settings.register_profile(\"long\", max_examples=1000)\n>>> settings.register_profile(\"fast\", max_examples=10)\n>>> settings.register_profile(\"debug\", max_examples=10, verbosity=Verbosity.verbose)\n>>> settings.load_profile(os.getenv(\"HYPOTHESIS_PROFILE\", \"default\"))\n```\n\nIf using pytest, you can also easily se"
  },
  {
    "book_name": "settings.html",
    "abstract": "在pytest中通过命令行参数选择Hypothesis的设置配置文件。",
    "content": "lect the active profile with `--hypothesis-profile`:\n\n```bash\n$ pytest --hypothesis-profile fast\n```\n\nSee the Hypothesis pytest plugin."
  },
  {
    "book_name": "stateful.html",
    "abstract": "介绍了 Hypothesis 库中状态测试（Stateful tests）的基本概念，将其与标准的 `@given` 测试进行对比，并引出了基于规则的状态机（Rule-based state machines）作为处理复杂测试场景的方法。",
    "content": "Stateful tests\n\nNote\n\nSee also [How not to Die Hard with Hypothesis](https://hypothesis.works/articles/how-not-to-die-hard-with-hypothesis/) and [An Introduction to Rule-Based Stateful Testing](https://hypothesis.works/articles/rule-based-stateful-testing/).\n\nWith `@given`, your tests are still something that you mostly write yourself, with Hypothesis providing some data. With Hypothesis’s stateful testing, Hypothesis instead tries to generate not just data but entire tests. You specify a number of primitive actions that can be combined together, and then Hypothesis will try to find sequences of those actions that result in a failure.\n\nYou may not need stateful tests\n\nThe basic idea of stateful testing is to make Hypothesis choose actions as well as values for your test, and state machines are a great declarative way to do just that.\n\nFor simpler cases though, you might not need them at all—a standard test with `@given` might be enough, since you can use `data()` in branches or loops. In fact, that’s how the state machine explorer works internally. For more complex workloads though, where a higher level API comes into its own, keep reading!\n\nRule-based state machines\n\nA state machine is very similar to a normal `@given` based test in that it takes values drawn from strategies and passes them to a user defined test function, which may use assertions to check the system’s behavior. The key difference is that where `@given` based tests must be independent, rules can be chained together—a single test run may involve multiple rule invocations, which may interact in various ways."
  },
  {
    "book_name": "stateful.html",
    "abstract": "介绍 Hypothesis 中 Bundles 的概念、用途及其与实例变量的对比。",
    "content": "\n\nRules can take normal strategies as arguments, but normal strategies, with the exception of `runner()` and `data()`, cannot take into account the current state of the machine. This is where bundles come in.\n\nA rule can, in place of a normal strategy, take a `Bundle`. A `hypothesis.stateful.Bundle` is a named collection of generated values that can be reused by other operations in the test. They are populated with the results of rules, and may be used as arguments to rules, allowing data to flow from one rule to another, and rules to work on the results of previous computations or actions.\n\nSpecifically, a rule that specifies `target=a_bundle` will cause its return value to be added to that bundle. A rule that specifies `an_argument=a_bundle` as a strategy will draw a value from that bundle. A rule can also specify that an argument chooses a value from a bundle and removes that value by using `consumes()` as in `an_argument=consumes(a_bundle)`.\n\nNote\n\nThere is some overlap between what you can do with Bundles and what you can do with instance variables. Both represent state that rules can manipulate. If you do not need to draw values that depend on the machine’s state, you can simply use instance variables. If you do need to draw values that depend on the machine’s state, Bundles provide a fairly straightforward way to do this. If you need rules that draw values that depend on the machine’s state in some more complicated way, you will have to abandon bundles. You can use `runner()` and `.flatmap()` to access the instance from a rule: the strategy `runner().flatmap(lambda self: sampled_from(self.a_list))` will draw from the instance variable `a_list`. If you need something more complicated still, you can use `data()` to draw data from the instance (or anywhere else) based on logic in the rule."
  },
  {
    "book_name": "stateful.html",
    "abstract": "Hypothesis规则状态机示例：通过规则状态机测试Hypothesis示例数据库实现，将其与内存中的字典模型进行行为对比。",
    "content": "\n\nThe following rule based state machine example is a simplified version of a test for Hypothesis’s example database implementation. An example database maps keys to sets of values, and in this test we compare one implementation of it to a simplified in memory model of its behaviour, which just stores the same values in a Python `dict`."
  },
  {
    "book_name": "stateful.html",
    "abstract": "介绍了一个用于比较真实数据库与内存模型行为的测试类 DatabaseComparison，其通过 Hypothesis 的状态机和 Bundle 机制生成并操作键值对。",
    "content": " The test then runs operations against both the real database and the in-memory representation of it and looks for discrepancies in their behaviour.\n\n```python\nimport shutil\nimport tempfile\nfrom collections import defaultdict\nimport hypothesis.strategies as st\nfrom hypothesis.database import DirectoryBasedExampleDatabase\nfrom hypothesis.stateful import Bundle, RuleBasedStateMachine, rule\n\nclass DatabaseComparison(RuleBasedStateMachine):\n    def __init__(self):\n        super().__init__()\n        self.tempd = tempfile.mkdtemp()\n        self.database = DirectoryBasedExampleDatabase(self.tempd)\n        self.model = defaultdict(set)\n\n    keys = Bundle(\"keys\")\n    values = Bundle(\"values\")\n\n    @rule(target=keys, k=st.binary())\n    def add_key(self, k):\n        return k\n\n    @rule(target=values, v=st.binary())\n    def add_value(self, v):\n        return v\n\n    @rule(k=keys, v=values)\n    def save(self, k, v):\n        self.model[k].add(v)\n        self.database.save(k, v)\n\n    @rule(k=keys, v=values)\n    def delete(self, k, v):\n        self.model[k].discard(v)\n        self.database.delete(k, v)\n\n    @rule(k=keys)\n    def values_agree(self, k):\n        assert set(self.database.fetch(k)) == self.model[k]\n\n    def teardown(self):\n        shutil.rmtree(self.tempd)\n\nTestDBComparison = DatabaseComparison.TestCase\n```\n\nIn this we declare two bundles—one for keys, and one for values. We have two trivial rules which just populate th"
  },
  {
    "book_name": "stateful.html",
    "abstract": "详细说明了 DatabaseComparison 状态机中非平凡规则（save, delete, values_agree）的功能，并解释了使用 Bundle 的优势在于促进操作复用相同的键和值。同时展示了如何将其集成到测试套件中以及失败时的输出示例。",
    "content": "em with data (`k` and `v`), and three non-trivial rules: `save` saves a value under a key and `delete` removes a value from a key, in both cases also updating the model of what should be in the database. `values_agree` then checks that the contents of the database agrees with the model for a particular key.\n\nNote\n\nWhile this could have been simplified by not using bundles, generating keys and values directly in the `save` and `delete` rules, using bundles encourages Hypothesis to choose the same keys and values for multiple operations. The bundle operations establish a “universe” of keys and values that are used in the rules.\n\nWe can now integrate this into our test suite by getting a unittest `TestCase` from it:\n\n```python\nTestTrees = DatabaseComparison.TestCase\n# Or just run with pytest's unittest support\nif __name__ == \"__main__\":\n    unittest.main()\n```\n\nThis test currently passes, but if we comment out the line where we call `self.model[k].discard(v)`, we would see the following output when run under pytest:\n\n```\nAssertionError: assert set() == {b''}\n------------ Hypothesis ------------\nstate = DatabaseComparison()\nvar1 = state.add_key(k=b'')\nvar2 = state.add_value(v=var1)\nstate.save(k=var1, v=var2)\nstate.delete(k=var1, v=var2)\nstate.values_agree(k=var1)\nstate.teardown()\n```\n\nNote how it’s printed out a very short program that will demonstrate the problem. The output from a rule based state machine should generally be pretty close to Python code—if you have custom `repr` implementations that don’t return valid Python then it might not be, but most of the time you should just be able to copy and paste the code into a test to reproduce it."
  },
  {
    "book_name": "stateful.html",
    "abstract": "(part 1/2) 介绍了如何通过设置 TestCase 的 settings 对象来控制测试行为，并详细阐述了规则（Rules）的定义、限制和使用方法，以及初始化（Initializes）作为一种特殊规则的作用和典型应用场景。",
    "content": "\n\nYou can control the detailed behaviour with a settings object on the `TestCase` (this is a normal hypothesis settings object using the defaults at the time the `TestCase` class was first referenced). For example if you wanted to run fewer examples with larger programs you could change the settings to:\n\n```python\nDatabaseComparison.TestCase.settings = settings(\n    max_examples=50, stateful_step_count=100\n)\n```\n\nWhich doubles the number of steps each program runs and halves the number of test cases that will be run.\n\nRules\n\nAs said earlier, rules are the most common feature used in `RuleBasedStateMachine`. They are defined by applying the `rule()` decorator on a function. Note that `RuleBasedStateMachine` must have at least one rule defined and that a single function cannot be used to define multiple rules (this is to avoid having multiple rules doing the same things). Due to the stateful execution method, rules generally cannot take arguments from other sources such as fixtures or `pytest.mark.parametrize`—consider providing them via a strategy such as `sampled_from()` instead.\n\nInitializes\n\nInitializes are a special case of rules, which are guaranteed to be run exactly once before any normal rule is called. Note if multiple initialize rules are defined, they will all be called but in any order, and that order will vary from run to run.\n\nInitializes are typically useful to populate bundles:\n\n```python\nimport hypothesis.strategies as st\nfrom hypothesis.stateful import Bundle, RuleBasedStateMachine, initialize, rule\n\nname_strategy = st.text(min_size=1).filter(lambda x: \"/\" not in x)\n\nclass NumberModifier(RuleBasedStateMachine):\n    folders = Bundle(\"folders\")\n    files = Bundle(\"files\")\n\n    @initialize(target=folders)\n    def init_folders(self):\n        return \"/\"\n\n    @rule(target=folders, parent=folders, name=name_strategy)\n    def create_folder(self, parent, name):\n        return f\"{parent}/{n"
  },
  {
    "book_name": "strategies.html",
    "abstract": "Hypothesis策略参考文档的引言和基本原语部分，介绍了策略的概念、用法以及`none()`、`nothing()`和`just(value)`三个基础策略。",
    "content": "Strategies Reference\n====================\n\nStrategies are the way Hypothesis describes the values for `@given` to generate. For instance, passing the strategy `st.lists(st.integers(), min_size=1)` to `@given` tells Hypothesis to generate lists of integers with at least one element.\n\nThis reference page lists all of Hypothesis’ first-party functions which return a strategy. There are also many provided by third-party libraries. Note that we often say “strategy” when we mean “function returning a strategy”; it’s usually clear from context which one we mean.\n\nStrategies can be passed to other strategies as arguments, combined using combinator strategies, or modified using `.filter()`, `.map()`, or `.flatmap()`.\n\nPrimitives\n----------\n\n- `hypothesis.strategies.none()` [source]_\n  Return a strategy which only generates `None`.\n  Examples from this strategy do not shrink (because there is only one)."
  },
  {
    "book_name": "strategies.html",
    "abstract": "Hypothesis策略库中用于生成固定值或无值的策略，以及整数生成策略的介绍。",
    "content": "\n\n- `hypothesis.strategies.nothing()` [source]_\n  This strategy never successfully draws a value and will always reject on an attempt to draw.\n  Examples from this strategy do not shrink (because there are none).\n\n- `hypothesis.strategies.just(value)` [source]_\n  Return a strategy which only generates `value`.\n\n  .. note::\n     `value` is not copied. Be wary of using mutable values.\n     If `value` is the result of a callable, you can use `builds(callable)` instead of `just(callable())` to get a fresh value each time.\n\n  Examples from this strategy do not shrink (because there is only one).\n\nNumeric\n-------\n\nSee also\n~~~~~~~~\n\nSee also the separate sections for Numpy strategies, Pandas strategies, and Array API strategies.\n\n- `hypothesis.strategies.integers(min_value=None, max_value=None)` [source]_\n  Returns a strategy which generates integers.\n  If `min_value` is not None then all values will be >= `min_value`. If `max_value` is not None then all values will be <= `max_value`.\n  Examples from this strategy will shrink towards zero, and negative values will also shrink towards positive (i.e. -n may be replaced by +n)."
  },
  {
    "book_name": "strategies.html",
    "abstract": "(part 1/2) 介绍了 hypothesis.strategies.floats 策略的参数、行为和约束，包括值域限制、特殊值（NaN, infinity, subnormal）的处理、精度宽度以及区间端点的排除规则。",
    "content": "\n\n- `hypothesis.strategies.floats(min_value=None, max_value=None, *, allow_nan=None, allow_infinity=None, allow_subnormal=None, width=64, exclude_min=False, exclude_max=False)` [source]_\n  Returns a strategy which generates floats.\n\n  If `min_value` is not None, all values will be >= `min_value` (or > `min_value` if `exclude_min`). If `max_value` is not None, all values will be <= `max_value` (or < `max_value` if `exclude_max`).\n\n  If `min_value` or `max_value` is not None, it is an error to enable `allow_nan`. If both `min_value` and `max_value` are not None, it is an error to enable `allow_infinity`. If inferred values range does not include subnormal values, it is an error to enable `allow_subnormal`.\n\n  Where not explicitly ruled out by the bounds, subnormals, infinities, and NaNs are possible values generated by this strategy.\n\n  The `width` argument specifies the maximum number of bits of precision required to represent the generated float. Valid values are 16, 32, or 64. Passing `width=32` will still use the builtin 64-bit `float` class, but always for values which can be exactly represented as a 32-bit float.\n\n  The `exclude_min` and `exclude_max` argument can be used to generate numbers from open or half-open intervals, by excluding the respective endpoints. Excluding either signed zero will also exclude the other. Attempting to exclude an endpoint which is `None` will raise an error; use `allow_infinity=False` to generate finite floats. You can however use e.g. `min_value=-math.inf, exclude_min=True` to exclude only one infinite endpoint.\n\n  Examples from this strategy have a complicated and hard to explain shrinking behaviour, but it tries to improve “human readability”. Finite numbers will be preferred to infinity and infinity will be preferred to NaN."
  },
  {
    "book_name": "strategies.html",
    "abstract": "文档片段：hypothesis库中complex_numbers策略的函数签名。",
    "content": "\n\n- `hypothesis.strategies.complex_numbers(*, min_magnitude=0, max_magnitude=None, allow_infinity=None, allow_nan=None, allow_subnormal=True, width=128)` [source]_"
  },
  {
    "book_name": "suppress-healthchecks.html",
    "abstract": "介绍了如何通过注册和加载设置配置文件来全局禁用特定的 Hypothesis 健康检查（HealthCheck），并说明了本地设置会覆盖全局配置。",
    "content": "Suppress a health check everywhere\n==================================\n\nHypothesis sometimes raises a `HealthCheck` to indicate that your test may be less effective than you expect, slower than you expect, unlikely to generate effective examples, or otherwise has silently degraded performance.\n\nWhile `HealthCheck` can be useful to proactively identify issues, you may not care about certain classes of them. If you want to disable a `HealthCheck` everywhere, you can register and load a settings profile with `register_profile()` and `load_profile()`. Place the following code in any file which is loaded before running your tests (or in `conftest.py`, if using pytest):\n\n```python\nfrom hypothesis import HealthCheck, settings\nsettings.register_profile(\n    \"my_profile\", suppress_health_check=[HealthCheck.filter_too_much]\n)\nsettings.load_profile(\"my_profile\")\n```\n\nThis profile in particular suppresses the `HealthCheck.filter_too_much` health check for all tests. The exception is if a test has a `@settings` which explicitly sets a different value for `suppress_health_check`, in which case the profile value will be overridden by the local settings value."
  },
  {
    "book_name": "suppress-healthchecks.html",
    "abstract": "文章开头强调应谨慎抑制健康检查，并提供了全局抑制所有健康检查的代码示例。",
    "content": "\n\nI want to suppress all health checks!\n=====================================\n\n**Warning:** We strongly recommend that you suppress health checks as you encounter them, rather than using a blanket suppression. Several health checks check for subtle interactions that may save you hours of debugging, such as `HealthCheck.function_scoped_fixture` and `HealthCheck.differing_executors`.\n\nIf you really want to suppress all health checks, for instance to speed up interactive prototyping, you can:\n\n```python\nfrom hypothesis import HealthCheck, settings\nsettings.register_profile(\"my_profile\", suppress_health_check=list(HealthCheck))\nsettings.load_profile(\"my_profile\")"
  },
  {
    "book_name": "type-strategies.html",
    "abstract": "介绍了如何为 Hypothesis 策略（strategies）编写类型提示，包括使用 SearchStrategy[T] 作为返回策略的函数的返回类型，并通过代码示例展示了基本用法。",
    "content": "Write type hints for strategies\n================================\n\nHypothesis provides type hints for all strategies and functions which return a strategy:\n\n```python\nfrom hypothesis import strategies as st\nreveal_type(st.integers())\n# SearchStrategy[int]\nreveal_type(st.lists(st.integers()))\n# SearchStrategy[list[int]]\n```\n\n`SearchStrategy` is the type of a strategy. It is parametrized by the type of the example it generates. You can use it to write type hints for your functions which return a strategy:"
  },
  {
    "book_name": "type-strategies.html",
    "abstract": "通过代码示例详细说明了如何为返回数值策略的函数编写类型提示，并阐明了策略本身（SearchStrategy[int]）与返回策略的函数（Callable[..., SearchStrategy[int]]）之间的类型区别。",
    "content": "\n\n```python\nfrom hypothesis import strategies as st\nfrom hypothesis.strategies import SearchStrategy\n\n# returns a strategy for \"normal\" numbers\ndef numbers() -> SearchStrategy[int | float]:\n    return st.integers() | st.floats(allow_nan=False, allow_infinity=False)\n```\n\nIt’s worth pointing out the distinction between a strategy, and a function that returns a strategy. `integers()` is a function which returns a strategy, and that strategy has type `SearchStrategy[int]`. The function `st.integers` therefore has type `Callable[..., SearchStrategy[int]]`, while the value `s = st.integers()` has type `SearchStrategy[int]`."
  },
  {
    "book_name": "type-strategies.html",
    "abstract": "说明了在使用 @composite 装饰器定义策略时，其类型提示应直接使用该策略生成值的类型（如 tuple[int, int]），而不是 SearchStrategy。",
    "content": "\n\nType hints for `@composite`\n---------------------------\n\nWhen writing type hints for strategies defined with `@composite`, use the type of the returned value (not `SearchStrategy`):\n\n```python\n@st.composite\ndef ordered_pairs(draw) -> tuple[int, int]:\n    n1 = draw(st.integers())\n    n2 = draw(st.integers(min_value=n1))\n    return (n1, n2)"
  },
  {
    "book_name": "type-strategies.html",
    "abstract": "解释了 SearchStrategy 的协变（covariant）特性，即如果类型 B 是 A 的子类型，那么 SearchStrategy[B] 也是 SearchStrategy[A] 的子类型，并用 Dog 和 Animal 的例子进行了说明。",
    "content": "\n```\n\nType variance of `SearchStrategy`\n--------------------------------\n\n`SearchStrategy` is covariant, meaning that if `B < A` then `SearchStrategy[B] < SearchStrategy[A]`. In other words, the strategy `st.from_type(Dog)` is a subtype of the strategy `st.from_type(Animal)`."
  },
  {
    "book_name": "adapting-strategies.html",
    "abstract": "介绍如何通过 .map() 方法对策略进行简单转换，例如将整数列表生成策略转换为有序列表生成策略。",
    "content": "Adapting strategies\n===================\n\nThis page discusses ways to adapt strategies to your needs, either by transforming them inline with `.map()`, or filtering out unwanted inputs with `.filter()` and `assume()`.\n\nMapping strategy inputs\n-----------------------\n\nSometimes you want to apply a simple transformation to a strategy. For instance, we know that we can generate lists of integers with `lists(integers())`. But maybe we wanted to instead generate sorted lists. We could use an inline `.map()` to achieve this:\n\nIn general, `strategy.map(f)` returns a new strategy which transforms all the examples generated by `strategy` by calling `f` on them."
  },
  {
    "book_name": "adapting-strategies.html",
    "abstract": "介绍了Hypothesis库中使用.filter()方法来过滤策略生成的输入，以排除不满足条件的测试用例，例如通过过滤掉零值来避免模运算中的ZeroDivisionError。",
    "content": "\n\nFiltering strategy inputs\n-------------------------\n\nMany strategies in Hypothesis offer some control over the kinds of values that get generated. For instance, `integers(min_value=0)` generates positive integers, and `integers(100, 200)` generates integers between 100 and 200.\n\nSometimes, you need more control than this. The inputs from a strategy may not match exactly what you need, and you just need to filter out a few bad cases.\n\nFor instance, suppose we have written a simple test involving the modulo operator `%`:\n\n```python\nfrom hypothesis import given, strategies as st\n\n@given(st.integers(), st.integers())\ndef test_remainder_magnitude(a, b):\n    # the remainder after division is always less than\n    # the divisor\n    assert abs(a % b) < abs(b)\n```\n\nHypothesis will quickly report a failure for this test: `ZeroDivisionError: integer modulo by zero`. Just like division, modulo isn’t defined for 0. The case of `b == 0` isn’t interesting for the test, and we would like to get rid of it.\n\nThe best way to do this is with the `.filter()` method:\n\n```python\nfrom hypothesis import assume, given, strategies as st\n\n@given(st.integers(), st.integers().filter(lambda n: n != 0))\ndef test_remainder_magnitude(a, b):\n    # b is guaranteed to be nonzero here, thanks to the filter\n    assert abs(a % b) < abs(b)\n```\n\nThis test now"
  },
  {
    "book_name": "adapting-strategies.html",
    "abstract": "介绍了Hypothesis库中用于筛选测试输入的两种方法：`.filter()`和`assume()`。`.filter()`作用于单个策略，而`assume()`可用于基于任意条件跳过整个测试用例。",
    "content": " passes cleanly.\n\nCalling `.filter()` on a strategy creates a new strategy with that filter applied at generation-time. For instance, `integers().filter(lambda n: n != 0)` is a strategy which generates nonzero integers.\n\nAssuming away test cases\n------------------------\n\n`.filter()` lets you filter test inputs from a single strategy. Hypothesis also provides an `assume()` function for when you need"
  },
  {
    "book_name": "adapting-strategies.html",
    "abstract": "介绍了Hypothesis库中`assume()`函数的用途，它可以根据任意条件跳过测试用例，并通过一个检查取余运算结果绝对值的示例展示了其用法。",
    "content": " to filter an entire test case, based on an arbitrary condition.\n\nThe `assume()` function skips test cases where some condition evaluates to `True`. You can use it anywhere in your test. We could have expressed our `.filter()` example above using `assume()` as well:\n\n```python\nfrom hypothesis import assume, given, strategies as st\n\n@given(st.integers(), st.integers())\ndef test_remainder_magnitude(a, b):\n    assume(b != 0)\n    # b will be nonzero here\n    assert abs(a % b) < abs(b)"
  },
  {
    "book_name": "adapting-strategies.html",
    "abstract": "对比了`assume()`和`.filter()`的使用场景，建议优先使用`.filter()`以获得更高效的采样，并指出对于无法用`.filter()`表达的复杂关系应使用`assume()`。",
    "content": "\n```\n\n`assume()` vs `.filter()`\n-------------------------\n\nWhere possible, you should use `.filter()`. Hypothesis can often rewrite simple filters into more efficient sampling methods than rejection sampling, and will retry filters several times instead of aborting the entire test case (as with `assume()`).\n\nFor more complex relationships that can’t be expressed with `.filter()`, use `assume()`."
  },
  {
    "book_name": "adapting-strategies.html",
    "abstract": "(part 1/2) 通过一个关于整除运算的测试用例，展示了如何结合使用`assume()`和`.filter()`来处理多个过滤条件，将简单的非零条件用`.filter()`实现。",
    "content": "\n\nHere’s an example of a test where we want to filter out two different types of examples:\n\n```python\nfrom hypothesis import assume, given, strategies as st\n\n@given(st.integers(), st.integers())\ndef test_floor_division_lossless_when_b_divides_a(a, b):\n    # we want to assume that:\n    # * b is nonzero, and\n    # * b divides a\n    assert (a // b) * b == a"
  },
  {
    "book_name": "adapting-strategies.html",
    "abstract": "(part 2/2) 说明了复杂的`a % b == 0`条件必须保留为`assume()`，因为它表达了`a`和`b`之间更复杂的关系。",
    "content": "\n```\n\nWe could start by using `assume()` for both:\n\n```python\nfrom hypothesis import assume, given, strategies as st\n\n@given(st.integers(), st.integers())\ndef test_floor_division_lossless_when_b_divides_a(a, b):\n    assume(b != 0)\n    assume(a % b == 0)\n    assert (a // b) * b == a\n```\n\nAnd then notice that the `b != 0` condition can be moved into the strategy definition as a `.filter()` call:\n\n```python\nfrom hypothesis import assume, given, strategies as st\n\n@given(st.integers(), st.integers().filter(lambda n: n != 0))\ndef test_floor_division_lossless_when_b_divides_a(a, b):\n    assume(a % b == 0)\n    assert (a // b) * b == a\n```\n\nHowever, the `a % b == 0` condition has to stay as an `assume()`, because it expresses a more complicated relationship between `a` and `b`."
  },
  {
    "book_name": "adding-notes.html",
    "abstract": "介绍了 Hypothesis 库中用于在测试失败时添加额外调试信息的 `note()` 函数，并通过一个打乱列表顺序的测试用例展示了其用法。同时简要提及了功能类似的 `event()` 函数，后者用于统计测试用例中的特定事件。",
    "content": "# Adding notes\n\nWhen a test fails, Hypothesis will normally print output that looks like this:\n\n```\nFalsifying example: test_a_thing(x=1, y=\"foo\")\n```\n\nSometimes you want to add some additional information to a failure, such as the output of some intermediate step in your test. The `note()` function lets you do this:\n\n```python\n>>> from hypothesis import given, note, strategies as st\n>>> @given(st.lists(st.integers()), st.randoms())\n... def test_shuffle_is_noop(ls, r):\n...     ls2 = list(ls)\n...     r.shuffle(ls2)\n...     note(f\"Shuffle: {ls2!r}\")\n...     assert ls == ls2\n...\n>>> try:\n...     test_shuffle_is_noop()\n... except AssertionError:\n...     print(\"ls != ls2\")\n...\nFalsifying example: test_shuffle_is_noop(ls=[0, 1], r=RandomWithSeed(1))\nShuffle: [1, 0]\nls != ls2\n```\n\n`note()` is like a print statement that gets attached to the falsifying example reported by Hypothesis. It’s also reported by observability, and shown for all examples (if `settings.verbosity` is set to `Verbosity.verbose` or higher).\n\n`event()` is a similar function which tells Hypothesis to count the number of test cases which reported each distinct value you pass, for inclusion in test statistics and observability reports."
  },
  {
    "book_name": "api.html",
    "abstract": "(part 1/2) 介绍了Hypothesis库中`@given`装饰器的基本用法，包括其作为主要入口点将函数转换为测试的功能，并详细说明了其参数可以是位置参数或关键字参数，以及相关的使用规则和示例。",
    "content": "# API Reference\n\nReference for non-strategy objects that are part of the Hypothesis API. For documentation on strategies, see the strategies reference.\n\n## `@given`\n\n- `hypothesis.given(*_given_arguments, **_given_kwargs)` [source]\n\nThe `@given` decorator turns a function into a Hypothesis test. This is the main entry point to Hypothesis.\n\nSee also the [Introduction to Hypothesis tutorial](#), which introduces defining Hypothesis tests with `@given`.\n\n### Arguments to `@given`\n\nArguments to `@given` may be either positional or keyword arguments:\n\n```python\n@given(st.integers(), st.floats())\ndef test_one(x, y):\n    pass\n\n@given(x=st.integers(), y=st.floats())\ndef test_two(x, y):\n    pass\n```\n\nIf using keyword arguments, the arguments may appear in any order, as with standard Python functions:\n\n```python\n# different order, but still equivalent to before\n@given(y=st.floats(), x=st.integers())\ndef test(x, y):\n    assert isinstance(x, int)\n    assert isinstance(y, float)\n```\n\nIf `@given` is provided fewer positional arguments than the decorated test, the test arguments are filled in on the right side, leaving the leftmost positional arguments unfilled:\n\n```python\n@given(st.integers(), st.floats())\ndef test(manual_string, y, z):\n    assert manual_string == \"x\"\n    assert isinstance(y, int)\n    assert isinstance(z, float)\n\n# `test` is now a callable which takes one argument `manual_string`\ntest(\"x\")  # or equivalently: test(manual_string=\"x\")\n```\n\nThe reason for this “from the right” behavior is to support using `@given` with instance methods, by automatically passing through `self`:\n\n```python\nclass MyTest(TestCase):\n    @given(st.integers())\n    def test(self, x):\n        assert isinstance(self, MyTest)\n        assert isinstance(x, int)\n```\n\nIf (and only if) using keyword arguments, `@given` may be combined with `**kwargs` or `*args`.\n\nIt is an error to:"
  },
  {
    "book_name": "api.html",
    "abstract": "(part 2/2) 继续介绍`@given`装饰器的使用限制，并引入了`hypothesis.infer`（即`...`）的概念，用于从类型注解推断策略。同时解释了`builds()`和`@given`在策略推断上的不同行为及各自的限制。",
    "content": "\n- Mix positional and keyword arguments to `@given`.\n- Use `@given` with a function that has a default value for an argument.\n- Use `@given` with positional arguments with a function that uses `*args`, `**kwargs`, or keyword-only arguments.\n\nThe function returned by `given` has all the same arguments as the original test, minus those that are filled in by `@given`. See the notes on framework compatibility for how this interacts with features of other testing libraries, such as pytest fixtures.\n\n## `hypothesis.infer`\n\nAn alias for `...` (`Ellipsis`). `infer` can be passed to `@given` or `builds()` to indicate that a strategy for that parameter should be inferred from its type annotations.\n\nIn all cases, using `infer` is equivalent to using `...`.\n\n### Inferred strategies\n\nIn some cases, Hypothesis can work out what to do when you omit arguments. This is based on introspection, not magic, and therefore has well-defined limits.\n\n`builds()` will check the signature of the target (using `inspect.signature()`). If there are required arguments with type annotations and no strategy was passed to `builds()`, `from_type()` is used to fill them in. You can also pass the value `...` (`Ellipsis`) as a keyword argument, to force this inference for arguments with a default value.\n\n`@given` does not perform any implicit inference for required arguments, as this would break compatibility with pytest fixtures. `...` (`Ellipsis`), can be used as a keyword argument to explicitly fill in an argument from its type annotation. You can also use the `infer` alias if writing a literal `...` seems too weird.\n\n`@given(...)` can also be specified to fill all arguments from their type annotations.\n\n### Limitations\n\nHypo"
  },
  {
    "book_name": "api.html",
    "abstract": "(part 1/2) 讨论了Hypothesis在类型注解推断上的限制，并介绍了`@example`装饰器，用于向测试添加显式输入，这些输入会在生成随机数据前被优先尝试，常用于复现失败用例。",
    "content": "thesis does not inspect PEP 484 type comments at runtime. While `from_type()` will work as usual, inference in `builds()` and `@given` will only work if you manually create the `__annotations__` attribute (e.g. by using `@annotations(...)` and `@returns(...)` decorators).\n\nThe `typing` module changes between different Python releases, including at minor versions. These are all supported on a best-effort basis, but you may encounter problems. Please report them to us, and consider updating to a newer version of Python as a workaround.\n\n## Explicit inputs\n\nSee also the [Replaying failed tests tutorial](#), which discusses using explicit inputs to reproduce failures.\n\n### `class hypothesis.example(*args, **kwargs)` [source]\n\nAdd an explicit input to a Hypothesis test, which Hypothesis will always try before generating random inputs. This combines the randomized nature of Hypothesis generation with a traditional parametrized test.\n\nFor example:\n\n```python\n@example(\"Hello World\")\n@example(\"some string with special significance\")\n@given(st.text())\ndef test_strings(s):\n    ...\n```\n\nwill call `test_strings(\"Hello World\")` and `test_strings(\"some string with special significance\")` before generating any random inputs.\n\n`@example` may be placed in any order relative to `@given` and `@settings`.\n\nExplicit inputs from `@example` are run in the `Phase.explicit` phase. Explicit inputs do not count towards `settings.max_examples`. Note that explicit inputs added by `@example` do not shrink. If an explicit input fails, Hypothesis will stop and report the failure without generating any random inputs.\n\n`@example` can also be used to easily reproduce a failure. For instance, if Hypothesis reports that `f(n=[0, math.nan])` fails, you can add `@example(n=[0, math.nan])` to your test to quickly reproduce that failure.\n\n### Arguments to `@example`\n\nArguments to `@example` have the same behavior and restrictions as arguments to `@given`. This means they may be either positional or keyword"
  },
  {
    "book_name": "api.html",
    "abstract": "(part 2/2) 详细说明了`@example`的参数用法，并介绍了其两个辅助方法：`.xfail()`用于标记预期会失败的示例，以及`.via()`用于为示例附加来源标签以支持自文档化。",
    "content": " arguments (but not both in the same `@example`):\n\n```python\n@example(1, 2)\n@example(x=1, y=2)\n@given(st.integers(), st.integers())\ndef test(x, y):\n    pass\n```\n\nNoting that while arguments to `@given` are strategies (like `integers()`), arguments to `@example` are values instead (like `1`).\n\nSee the Arguments to `@given` section for full details.\n\n### `example.xfail(condition=True, *, reason='', raises=<class 'BaseException'>)`\n\nMark this example as an expected failure, similarly to `pytest.mark.xfail(strict=True)`.\n\nExpected-failing examples allow you to check that your test does fail on some examples, and therefore build confidence that passing tests are because your code is working, not because the test is missing something.\n\n**Note**: Expected-failing examples are handled separately from those generated by strategies, so you should usually ensure that there is no overlap.\n\n```python\n@example(x=1, y=0).xfail(raises=ZeroDivisionError)\n@given(x=st.just(1), y=st.integers())  # Missing `.filter(bool)`!\ndef test_fraction(x, y):\n    # This test will try the explicit example and see it fail as\n    # expected, then go on to generate more examples from the\n    # strategy. If we happen to generate y=0, the test will fail\n    # because only the explicit example is treated as xfailing.\n    x / y\n```\n\n### `example.via(whence, /)` [source]\n\nAttach a machine-readable label noting what the origin of this example was.\n\n`example.via` is completely optional and does not change runtime behavior. `example.via` is intended to support self-documenting behavior, as well as tooling which might a"
  },
  {
    "book_name": "api.html",
    "abstract": "(part 1/2) 介绍了用于复现测试失败的`hypothesis.reproduce_failure`装饰器，以及用于固定随机种子以确保测试可重现的`hypothesis.seed`装饰器，并说明了它们的使用场景和注意事项。",
    "content": "dd (or remove) `@example` decorators automatically. For example:\n\n**Note**: HypoFuzz uses `example.via` to tag examples in the patch of its high-coverage set of explicit inputs, on the patches page.\n\n## Reproducing inputs\n\nSee also the [Replaying failed tests tutorial](#).\n\n### `hypothesis.reproduce_failure(version, blob)` [source]\n\nRun the example corresponding to the binary `blob` in order to reproduce a failure. `blob` is a serialized version of the internal input representation of Hypothesis.\n\nA test decorated with `@reproduce_failure` always runs exactly one example, which is expected to cause a failure. If the provided `blob` does not cause a failure, Hypothesis will raise `DidNotReproduce`.\n\nHypothesis will print an `@reproduce_failure` decorator if `settings.print_blob` is `True` (which is the default in CI).\n\n`@reproduce_failure` is intended to be temporarily added to your test suite in order to reproduce a failure. It is not intended to be a permanent addition to your test suite. Because of this, no compatibility guarantees are made across Hypothesis versions, and `@reproduce_failure` will error if used on a different Hypothesis version than it was created for.\n\nSee also the [Replaying failed tests tutorial](#).\n\n### `hypothesis.seed(seed)` [source]\n\nSeed the randomness for this test.\n\n`seed` may be any hashable object. No exact meaning for `seed` is provided other than that for a fixed seed value Hypothesis will produce the same examples (assuming that there are no other sources of nondeterminism, such as timing, hash randomization, or external state).\n\nFor example, the following test function and `RuleBasedStateMachine` will each generate the same series of examples each time they are executed:\n\n```python\n@seed(1234)\n@given(st.integers())\ndef test(n):\n    ...\n\n@seed(6789)\nclass MyMachine(RuleBasedStateMachine):\n    ...\n```\n\nIf using pytest, you can alternatively pass `--hypothesis-seed` on the command line.\n\nSetting a seed overrides `settings.derandomize"
  },
  {
    "book_name": "api.html",
    "abstract": "(part 2/2) 介绍了Hypothesis的控制功能，包括`assume()`用于过滤无效示例，以及`event()`用于记录自定义事件以在测试统计中提供更详细的运行时信息。",
    "content": "`, which is designed to enable deterministic CI tests rather than reproducing observed failures.\n\nHypothesis will only print the seed which would reproduce a failure if a test fails in an unexpected way, for instance inside Hypothesis internals.\n\n## Control\n\nFunctions that can be called from anywhere inside a test, to either modify how Hypothesis treats the current test case, or to give Hypothesis more information about the current test case.\n\n### `hypothesis.assume(condition)` [source]\n\nCalling `assume` is like an assert that marks the example as bad, rather than failing the test.\n\nThis allows you to specify properties that you assume will be true, and let Hypothesis try to avoid similar examples in future.\n\n### `hypothesis.event(value, payload='')` [source]\n\nRecord an event that occurred during this test. Statistics on the number of test runs with each event will be reported at the end if you run Hypothesis in statistics reporting mode.\n\nEvent values should be strings or convertible to them. If an optional payload is given, it will be included in the string for Test statistics.\n\nYou can mark custom events in a test using `event()`:\n\n```python\nfrom hypothesis import event, given, strategies as st\n\n@given(st.integers().filter(lambda x: x % 2 == 0))\ndef test_even_integers(i):\n    event(f\"i mod 3 = {i%3}\")\n```\n\nThese events appear in observability output, as well as the output of our pytest plugin when run with `--hypothesis-show-statistics`.For instance, in the latter case, you would see output like:\n\n```\ntest_even_integers:\n- during generate phase (0.09 seconds):\n  - Typical runtimes: < 1ms, ~ 59% in data generation\n  - 100 passing examples, 0 failing examples, 32 invalid examples\n  - Events:\n    * 54.55%, Retried draw from integers().filter(lambda x: x % 2 == 0) to satisfy filter\n    * 31.06%, i mod 3 = 2\n    * 28.79%, i mod 3 = 0\n    * 24.24%, Aborted test because unable to satisfy integers().filter(lambda x: x % 2 == 0)\n    * 15.91%, i mod 3 = 1\n- Stopped because"
  },
  {
    "book_name": "api.html",
    "abstract": "(part 1/2) 介绍了Hypothesis的定向属性测试（Targeted PBT）功能，它通过`target()`函数引导输入生成，以更有效地探索可能导致属性失效的输入空间，并附带了浮点数结合律测试的示例。",
    "content": " settings.max_examples=100\n```\n\nArguments to event can be any hashable type, but two events will be considered the same if they are the same when converted to a string with `str`.\n\n## Targeted property-based testing\n\nTargeted property-based testing combines the advantages of both search-based and property-based testing. Instead of being completely random, targeted PBT uses a search-based component to guide the input generation towards values that have a higher probability of falsifying a property. This explores the input space more effectively and requires fewer tests to find a bug or achieve a high confidence in the system being tested than random PBT. (Löscher and Sagonas)\n\nThis is not always a good idea - for example calculating the search metric might take time better spent running more uniformly-random test cases, or your target metric might accidentally lead Hypothesis away from bugs - but if there is a natural metric like “floating-point error”, “load factor” or “queue length”, we encourage you to experiment with targeted testing.\n\nWe recommend that users also skim the papers introducing targeted PBT; from ISSTA 2017 and ICST 2018. For the curious, the initial implementation in Hypothesis uses hill-climbing search via a mutating fuzzer, with some tactics inspired by simulated annealing to avoid getting stuck and endlessly mutating a local maximum.\n\n```python\nfrom hypothesis import given, strategies as st, target\n\n@given(st.floats(0, 1e100), st.floats(0, 1e100), st.floats(0, 1e100))\ndef test_associativity_with_target(a, b, c):\n    ab_c = (a + b) + c\n    a_bc = a + (b + c)\n    difference = abs(ab_c - a_bc)\n    target(difference)  # Without this, the test almost always passes\n    assert difference < 2.0\n```\n\n- `hypothesis.target(observation, *, label='')`[source]\n\nCalling this function with an `int` or `float` observation gives it feedback with which to guide our search for inputs that will cause an error, in addition to all the usual heuristics. Observations mu"
  },
  {
    "book_name": "api.html",
    "abstract": "(part 2/2) 详细说明了`target()`函数的使用，包括其对观测值的要求、可选的`label`参数用法，并介绍了`hypothesis.settings`类，它用于控制测试的各种行为参数。",
    "content": "st always be finite. Hypothesis will try to maximize the observed value over several examples; almost any metric will work so long as it makes sense to increase it. For example, `-abs(error)` is a metric that increases as `error` approaches zero.\n\nExample metrics:\n- Number of elements in a collection, or tasks in a queue\n- Mean or maximum runtime of a task (or both, if you use `label`)\n- Compression ratio for data (perhaps per-algorithm or per-level)\n- Number of steps taken by a state machine\n\nThe optional `label` argument can be used to distinguish between and therefore separately optimise distinct observations, such as the mean and standard deviation of a dataset. It is an error to call `target()` with any label more than once per test case.\n\n**Note**\n\nThe more examples you run, the better this technique works. As a rule of thumb, the targeting effect is noticeable above `max_examples=1000`, and immediately obvious by around ten thousand examples per label used by your test. Test statistics include the best score seen for each label, which can help avoid the threshold problem when the minimal example shrinks right down to the threshold of failure (issue #2180).\n\n## Settings\n\nSee also the tutorial for settings.\n\n- `class hypothesis.settings(parent=None, *, max_examples=not_set, derandomize=not_set, database=not_set, verbosity=not_set, phases=not_set, stateful_step_count=not_set, report_multiple_bugs=not_set, suppress_health_check=not_set, deadline=not_set, print_blob=not_set, backend=not_set)`\n\nA settings object controls the following aspects of test behavior: `max_examples`, `derandomize`, `database`, `verbosity`, `phases`, `stateful_step_count`, `report_multiple_bugs`, `suppress_health_check`, `deadline`, `print_blob`, and `backend`.\n\nA settings object can be applied as a decorator to a test function, in which case that test function will use those settings. A test may only have one settings object applied to it. A settings object can also be passed to `register_"
  },
  {
    "book_name": "api.html",
    "abstract": "(part 1/2) 介绍了Hypothesis设置（settings）的属性继承机制，以及两个内置配置文件`default`和`ci`。`ci`配置文件在持续集成环境中自动激活，并覆盖了`default`的部分设置。",
    "content": "profile()` or as a parent to another `settings`.\n\n### Attribute inheritance\n\nSettings objects are immutable once created. When a settings object is created, it uses the value specified for each attribute. Any attribute which is not specified will inherit from its value in the `parent` settings object. If `parent` is not passed, any attributes which are not specified will inherit from the current settings profile instead.\n\nFor instance, `settings(max_examples=10)` will have a `max_examples` of `10`, and the value of all other attributes will be equal to its value in the current settings profile.\n\nChanges made from activating a new settings profile with `load_profile()` will be reflected in settings objects created after the profile was loaded, but not in existing settings objects.\n\n### Built-in profiles\n\nWhile you can register additional profiles with `register_profile()`, Hypothesis comes with two built-in profiles: `default` and `ci`.\n\nBy default, the `default` profile is active. If the `CI` environment variable is set to any value, the `ci` profile is active by default. Hypothesis also automatically detects various vendor-specific CI environment variables.\n\nThe attributes of the currently active settings profile can be retrieved with `settings()` (so `settings().max_examples` is the currently active default for `settings.max_examples`).\n\nThe settings attributes for the built-in profiles are as follows:\n\n```python\ndefault = settings.register_profile(\n    \"default\",\n    max_examples=100,\n    derandomize=False,\n    database=not_set,  # see settings.database for the default database\n    verbosity=Verbosity.normal,\n    phases=tuple(Phase),\n    stateful_step_count=50,\n    report_multiple_bugs=True,\n    suppres"
  },
  {
    "book_name": "api.html",
    "abstract": "(part 2/2) 详细解释了`settings`的两个关键属性：`max_examples`控制测试的最大示例数量，以及`derandomize`用于通过哈希种子使测试结果可重现，并讨论了它们在不同场景下的应用。",
    "content": "s_health_check=(),\n    deadline=duration(milliseconds=200),\n    print_blob=False,\n    backend=\"hypothesis\",\n)\n\nci = settings.register_profile(\n    \"ci\",\n    parent=default,\n    derandomize=True,\n    deadline=None,\n    database=None,\n    print_blob=True,\n    suppress_health_check=[HealthCheck.too_slow],\n)\n```\n\nYou can replace either of the built-in profiles with `register_profile()`:\n\n```python\n# run more examples in CI\nsettings.register_profile(\n    \"ci\",\n    settings.get_profile(\"ci\"),\n    max_examples=1000,\n)\n```\n\n- **property max_examples**\n\nOnce this many satisfying examples have been considered without finding any counter-example, Hypothesis will stop looking.\n\nNote that we might call your test function fewer times if we find a bug early or can tell that we’ve exhausted the search space; or more if we discard some examples due to use of `.filter()`, `assume()`, or a few other things that can prevent the test case from completing successfully.\n\nThe default value is chosen to suit a workflow where the test will be part of a suite that is regularly executed locally or on a CI server, balancing total running time against the chance of missing a bug.\n\nIf you are writing one-off tests, running tens of thousands of examples is quite reasonable as Hypothesis may miss uncommon bugs with default settings. For very complex code, we have observed Hypothesis finding novel bugs after several million examples while testing SymPy. If you are running more than 100k examples for a test, consider using our integration for coverage-guided fuzzing - it really shines when given minutes or hours to run.\n\nThe default max examples is `100`.\n\n- **property derandomize**\n\nIf True, seed Hypothesis’ random number generator using a hash of the test function, so that every run will test the same set of examples until you update Hypothesis, Python, or the test"
  },
  {
    "book_name": "api.html",
    "abstract": "(part 1/2) 介绍了`settings`的`database`属性，用于持久化存储测试示例，以及`verbosity`属性，用于控制Hypothesis在运行时输出信息的详细程度，并给出了不同详细级别的示例。",
    "content": " function.\n\nThis allows you to check for regressions and look for bugs using separate settings profiles - for example running quick deterministic tests on every commit, and a longer non-deterministic nightly testing run.\n\nThe default is `False`. If running on CI, the default is `True` instead.\n\n- **property database**\n\nAn instance of `ExampleDatabase` that will be used to save examples to and load previous examples from.\n\nIf not set, a `DirectoryBasedExampleDatabase` is created in the current working directory under `.hypothesis/examples`. If this location is unusable, e.g. due to the lack of read or write permissions, Hypothesis will emit a warning and fall back to an `InMemoryExampleDatabase`.\n\nIf `None`, no storage will be used.\n\nSee the database documentation for a list of database classes, and how to define custom database classes.\n\n- **property verbosity**\n\nControl the verbosity level of Hypothesis messages.\n\nTo see what’s going on while Hypothesis runs your tests, you can turn up the verbosity setting.\n\n```python\n>>> from hypothesis import settings, Verbosity\n>>> from hypothesis.strategies import lists, integers\n>>> @given(lists(integers()))\n... @settings(verbosity=Verbosity.verbose)\n... def f(x):\n...     assert not any(x)\n...\n>>> f()\nTrying example: []\nFalsifying example: [-1198601713, -67, 116, -29578]\nShrunk example to [-1198601713]\nShrunk example to [-128]\nShrunk example to [32]\nShrunk example to [1]\n[1]\n```\n\nThe four levels are `Verbosity.quiet`, `Verbosity.normal`, `Verbosity.verbose`, and `Verbosity.debug`. `Verbosity.normal` is the default. For `Verbosity.quie"
  },
  {
    "book_name": "api.html",
    "abstract": "(part 2/2) 详细说明了`settings`的`phases`属性，它允许用户控制测试执行的不同逻辑阶段（如显式示例、重用、生成、目标化、收缩和解释），并解释了各阶段之间的依赖关系。",
    "content": "t`, Hypothesis will not print anything out, not even the final falsifying example. `Verbosity.debug` is basically `Verbosity.verbose` but a bit more so. You probably don’t want it.\n\nVerbosity can be passed either as a `Verbosity` enum value, or as the corresponding string value, or as the corresponding integer value. For example:\n\nIf you are using pytest, you may also need to disable output capturing for passing tests to see verbose output as tests run.\n\n- **property phases**\n\nControl which phases should be run.\n\nHypothesis divides tests into logically distinct phases.\n\n- `Phase.explicit`: Running explicit examples from `@example`.\n- `Phase.reuse`: Running examples from the database which previously failed.\nPhase.generate: Generating new random examples.\nPhase.target: Mutating examples for targeted property-based testing. Requires Phase.generate.\nPhase.shrink: Shrinking failing examples.\nPhase.explain: Attempting to explain why a failure occurred. Requires Phase.shrink.\n\nThe phases argument accepts a collection with any subset of these. E.g.\n`settings(phases=[Phase.generate, Phase.shrink])`\nwill generate new examples and shrink them, but will not run explicit examples or reuse previous failures, while `settings(phases=[Phase.explicit])`\nwill only run explicit examples from `@example`.\n\nPhases can be passed either as a `Phase` enum value, or as the corresponding string value. For example:\n\nFollowing the first failure, Hypothesis will (usually, depending on which `Phase` is enabled) track which lines of code are always run on failing but never on passing inputs. On 3.12+, this uses `sys.monitoring`, while 3.11 and earlier use `sys.settrace()`. For python 3.11 and earlier, we therefore automatically disable the explain phase on PyPy, or if you are using coverage or a debugger. If there are no clearly suspicious lines of code, we refuse the temptation to guess.\n\nAfter shrinking to a minimal failing example, Hypothesis will try to find parts of the example – e.g. separat"
  },
  {
    "book_name": "api.html",
    "abstract": "(part 1/2) 介绍了`settings`的多个属性：`stateful_step_count`控制状态测试的步骤数，`report_multiple_bugs`决定是否报告多个错误，`suppress_health_check`用于抑制健康检查警告。",
    "content": "e args to `@given` – which can vary freely without changing the result of that minimal failing example. If the automated experiments run without finding a passing variation, we leave a comment in the final report:\n```python\ntest_x_divided_by_y(\n    x=0,  # or any other generated value\n    y=0,\n)\n```\nJust remember that the lack of an explanation sometimes just means that Hypothesis couldn’t efficiently find one, not that no explanation (or simpler failing example) exists.\n\n- **property stateful_step_count**  \n  The maximum number of times to call an additional `@rule` method in stateful testing before we give up on finding a bug. Note that this setting is effectively multiplicative with max_examples, as each example will run for a maximum of `stateful_step_count` steps. The default stateful step count is `50`.\n\n- **property report_multiple_bugs**  \n  Because Hypothesis runs the test many times, it can sometimes find multiple bugs in a single run. Reporting all of them at once is usually very useful, but replacing the exceptions can occasionally clash with debuggers. If disabled, only the exception with the smallest minimal example is raised. The default value is `True`.\n\n- **property suppress_health_check**  \n  Suppress the given `HealthCheck` exceptions. Those health checks will not be raised by Hypothesis. To suppress all health checks, you can pass `suppress_health_check=list(HealthCheck)`. Health checks can be passed either as a `HealthCheck` enum value, or as the corresponding string value. For example: Health checks are proactive warnings, not correctness errors, so we encourage suppressing health checks where you have evaluated they will not pose a problem, or where you have evaluated that fixing the underlying issue is not worthwhile.  \n  See also: [Suppress a health check everywhere how-to](#).\n\n- **property deadline**  \n  The maximum allowed duration of an individual test case, in milliseconds. You can pass an integer, float, or timedelta. If `None`, the de"
  },
  {
    "book_name": "api.html",
    "abstract": "(part 2/2) 介绍了`settings`的`deadline`、`print_blob`和`backend`属性，并说明了用于管理配置文件的静态方法（`register_profile`, `get_profile`, `load_profile`）以及`Phase`枚举类。",
    "content": "adline is disabled entirely. We treat the deadline as a soft limit in some cases, where that would avoid flakiness due to timing variability. The default deadline is 200 milliseconds. If running on CI, the default is `None` instead.\n\n- **property print_blob**  \n  If set to `True`, Hypothesis will print code for failing examples that can be used with `@reproduce_failure` to reproduce the failing example. The default value is `False`. If running on CI, the default is `True` instead.\n\n- **property backend**  \n  **Warning**: EXPERIMENTAL AND UNSTABLE - see Alternative backends for Hypothesis.  \n  The importable name of a backend which Hypothesis should use to generate primitive types. We support heuristic-random, solver-based, and fuzzing-based backends.\n\n- **static register_profile(name, parent=None, \\*\\*kwargs)[source]**  \n  Register a settings object as a settings profile, under the name `name`. The `parent` and `kwargs` arguments to this method are as for `settings`. If a settings profile already exists under `name`, it will be overwritten. Registering a profile with the same name as the currently active profile will cause those changes to take effect in the active profile immediately, and do not require reloading the profile. Registered settings profiles can be retrieved later by name with `get_profile()`.\n\n- **static get_profile(name)[source]**  \n  Returns the settings profile registered under `name`. If no settings profile is registered under `name`, raises `InvalidArgument`.\n\n- **static load_profile(name)[source]**  \n  Makes the settings profile registered under `name` the active profile. If no settings profile is registered under `name`, raises `InvalidArgument`.\n\n- **class hypothesis.Phase(value)[source]**  \n  Options for the `settings.phases` argument to `@settings`.\n  - `explicit = 'explicit'`: Controls whether explicit examples are run.\n  - `reuse = 'reuse'`: Controls whether previous examples will be reused.\n  - `generate = 'generate'`: Controls whether ne"
  },
  {
    "book_name": "api.html",
    "abstract": "(part 1/2) 完整列出了`Phase`和`Verbosity`枚举类的所有选项及其作用，并开始介绍`HealthCheck`类，它用于在检测到测试性能问题时发出主动警告。",
    "content": "w examples will be generated.\n  - `target = 'target'`: Controls whether examples will be mutated for targeting.\n  - `shrink = 'shrink'`: Controls whether examples will be shrunk.\n  - `explain = 'explain'`: Controls whether Hypothesis attempts to explain test failures.  \n    The explain phase has two parts, each of which is best-effort - if Hypothesis can’t find a useful explanation, we’ll just print the minimal failing example.\n\n- **class hypothesis.Verbosity(value)[source]**  \n  Options for the `settings.verbosity` argument to `@settings`.\n  - `quiet = 'quiet'`: Hypothesis will not print any output, not even the final falsifying example.\n  - `normal = 'normal'`: Standard verbosity. Hypothesis will print the falsifying example, alongside any notes made with `note()` (only for the falsifying example).\n  - `verbose = 'verbose'`: Increased verbosity. In addition to everything in `Verbosity.normal`, Hypothesis will print each example as it tries it, as well as any notes made with `note()` for every example. Hypothesis will also print shrinking attempts.\n  - `debug = 'debug'`: Even more verbosity. Useful for debugging Hypothesis internals. You probably don’t want this.\n\n- **class hypothesis.HealthCheck(value)[source]**  \n  A `HealthCheck` is proactively raised by Hypothesis when Hypothesis detects that your test has performance problems, which may result in less rigorous testing than you expect. For example, if your test takes a long time to generate inputs, or filters away too many inputs using `assume()` or `.filter()`, Hypothesis will raise a corresponding health check. A health check is a proactive warning, not an error. We encourage suppressing health checks where you have evaluated they will not pose a problem, or where you have evaluated that fixing the underlying issue is not worthwhile. With the exception of `HealthCheck.function_scoped_fixture` and `HealthCheck.differing_executors`, all health checks warn about performance problems, not correctness errors.\n\n**D"
  },
  {
    "book_name": "api.html",
    "abstract": "(part 2/2) 详细说明了如何禁用健康检查，并列举了`HealthCheck`枚举的各个成员，包括针对正确性问题的检查（如`function_scoped_fixture`）和针对性能问题的检查（如`data_too_large`, `filter_too_much`等）。",
    "content": "isabling health checks**  \nHealth checks can be disabled by `settings.suppress_health_check`. To suppress all health checks, you can pass `suppress_health_check=list(HealthCheck)`.  \nSee also: [Suppress a health check everywhere how-to](#).\n\n**Correctness health checks**  \nSome health checks report potential correctness errors, rather than performance problems.  \n- `HealthCheck.function_scoped_fixture` indicates that a function-scoped pytest fixture is used by an `@given` test. Many Hypothesis users expect function-scoped fixtures to reset once per input, but they actually reset once per test. We proactively raise `HealthCheck.function_scoped_fixture` to ensure you have considered this case.  \n- `HealthCheck.differing_executors` indicates that the same `@given` test has been executed multiple times with multiple distinct executors. We recommend treating these particular health checks with more care, as suppressing them may result in an unsound test.\n\n- `data_too_large = 'data_too_large'`: Checks if too many examples are aborted for being too large. This is measured by the number of random choices that Hypothesis makes in order to generate something, not the size of the generated object. For example, choosing a 100MB object from a predefined list would take only a few bits, while generating 10KB of JSON from scratch might trigger this health check.\n- `filter_too_much = 'filter_too_much'`: Check for when the test is filtering out too many examples, either through use of `assume()` or `.filter()`, or occasionally for Hypothesis internal reasons.\n- `too_slow = 'too_slow'`: Check for when input generation is very slow. Since Hypothesis generates 100 (by default) inputs per test execution, a slowdown in generating each input can result in very slow tests overall.\n- `return_value = 'return_value'`: Deprecated; we always error if a test returns a non-None value.\n- `large_base_example = 'large_base_example'`: Checks if the smallest natural input to your test is very large. T"
  },
  {
    "book_name": "api.html",
    "abstract": "(part 1/2) 继续详细解释`HealthCheck`的各个成员，特别是`large_base_example`、`not_a_test_method`、`function_scoped_fixture`和`differing_executors`，并说明了它们各自检测的问题及推荐的解决方案。",
    "content": "his makes it difficult for Hypothesis to generate good inputs, especially when trying to shrink failing inputs.\n- `not_a_test_method = 'not_a_test_method'`: Deprecated; we always error if `@given` is applied to a method defined by `unittest.TestCase` (i.e. not a test).\n- `function_scoped_fixture = 'function_scoped_fixture'`: Checks if `@given` has been applied to a test with a pytest function-scoped fixture. Function-scoped fixtures run once for the whole function, not once per example, and this is usually not what you want. Because of this limitation, tests that need to set up or reset state for every example need to do so manually within the test itself, typically using an appropriate context manager. Suppress this health check only in the rare case that you are using a function-scoped fixture that does not need to be reset between individual examples, but for some reason you cannot use a wider fixture scope (e.g. session scope, module scope, class scope).This check requires the Hypothesis pytest plugin, which is enabled by default when running Hypothesis inside pytest.\n\n- `differing_executors = 'differing_executors'`  \n  Checks if `@given` has been applied to a test which is executed by different executors. If your test function is defined as a method on a class, that class will be your executor, and subclasses executing an inherited test is a common way for things to go wrong. The correct fix is often to bring the executor instance under the control of hypothesis by explicit parametrization over, or sampling from, subclasses, or to refactor so that `@given` is specified on leaf subclasses.\n\n- `nested_given = 'nested_given'`  \n  Checks if `@given` is used inside another `@given`. This results in quadratic generation and shrinking behavior, and can usually be expressed more cleanly by using `data()` to replace the inner `@given`. Nesting `@given` can be appropriate if you set appropriate limits for the quadratic behavior and cannot easily reexpress the inner funct"
  },
  {
    "book_name": "api.html",
    "abstract": "(part 2/2) 介绍了`HealthCheck.nested_given`的含义，并开始讲解Hypothesis的数据库（Database）功能，包括其作为缓存的角色、变更监听（change listening）扩展以及数据库方法的基本分类。",
    "content": "ion with `data()`. To suppress this health check, set `suppress_health_check=[HealthCheck.nested_given]` on the outer `@given`. Setting it on the inner `@given` has no effect. If you have more than one level of nesting, add a suppression for this health check to every `@given` except the innermost one.\n\n## Database\n\n- `class hypothesis.database.ExampleDatabase[source]`  \n  A Hypothesis database, for use in `settings.database`. Hypothesis automatically saves failures to the database set in `settings.database`. The next time the test is run, Hypothesis will replay any failures from the database in `settings.database` for that test (in `Phase.reuse`). The database is best thought of as a cache that you never need to invalidate. Entries may be transparently dropped when upgrading your Hypothesis version or changing your test. Do not rely on the database for correctness; to ensure Hypothesis always tries an input, use `@example`. A Hypothesis database is a simple mapping of bytes to sets of bytes. Hypothesis provides several concrete database subclasses. To write your own database class, see Write a custom Hypothesis database.\n\n## Change listening\n\nAn optional extension to `ExampleDatabase` is change listening. On databases which support change listening, calling `add_listener()` adds a function as a change listener, which will be called whenever a value is added, deleted, or moved inside the database. See `add_listener()` for details. All databases in Hypothesis support change listening. Custom database classes are not required to support change listening, though they will not be compatible with features that require change listening until they do so.\n\n> **Note**  \n> While no Hypothesis features currently require change listening, change listening is required by HypoFuzz.\n\n## Database methods\n\n**Required methods:**  \n**Optional methods:**  \n**Change listening methods:**\n\n- `abstract save(key, value)[source]`  \n  Save `value` under `key`. If `value` is already present in"
  },
  {
    "book_name": "api.html",
    "abstract": "(part 1/2) 详细列出了`ExampleDatabase`类的核心方法，包括必需的`save`和`delete`，可选的`move`，以及用于变更监听的`add_listener`、`remove_listener`、`_broadcast_change`和`_start_listening`。",
    "content": " `key`, silently do nothing.\n\n- `abstract delete(key, value)[source]`  \n  Remove `value` from `key`. If `value` is not present in `key`, silently do nothing.\n\n- `move(src, dest, value)[source]`  \n  Move `value` from key `src` to key `dest`. Equivalent to `delete(src, value)` followed by `save(dest, value)`, but may have a more efficient implementation. Note that `value` will be inserted at `dest` regardless of whether it is currently present at `src`.\n\n- `add_listener(f, /)[source]`  \n  Add a change listener. `f` will be called whenever a value is saved, deleted, or moved in the database. `f` can be called with two different event values:  \n  `(\"save\", (key, value))`  \n  `(\"delete\", (key, value))`  \n  where `key` and `value` are both `bytes`. There is no `move` event. Instead, a move is broadcasted as a `delete` event followed by a `save` event. For the `delete` event, `value` may be `None`. This might occur if the database knows that a deletion has occurred in `key`, but does not know what value was deleted.\n\n- `remove_listener(f, /)[source]`  \n  Removes `f` from the list of change listeners. If `f` is not in the list of change listeners, silently do nothing.\n\n- `_broadcast_change(event)[source]`  \n  Called when a value has been either added to or deleted from a key in the underlying database store. The possible values for `event` are:  \n  `(\"save\", (key, value))`  \n  `(\"delete\", (key, value))`  \n  `value` may be `None` for the `delete` event, indicating we know that some value was deleted under this key, but not its exact value. Note that you should not assume your instance is the only reference to the underlying database store. For example, if two instances of `DirectoryBasedExampleDatabase` reference the same directory, `_broadcast_change` should be called whenever a file is added or removed from the directory, even if that database was not responsible for changing the file.\n\n- `_start_listening()[source]`  \n  Called when the database adds a change listener, and"
  },
  {
    "book_name": "api.html",
    "abstract": "(part 2/2) 介绍了Hypothesis提供的几种具体数据库实现：`InMemoryExampleDatabase`（内存中）、`DirectoryBasedExampleDatabase`（基于目录的文件系统）和`GitHubArtifactDatabase`（从GitHub Actions产物加载）。",
    "content": " did not previously have any change listeners. Intended to allow databases to wait to start expensive listening operations until necessary. `_start_listening` and `_stop_listening` are guaranteed to alternate, so you do not need to handle the case of multiple consecutive `_start_listening` calls without an intermediate `_stop_listening` call.\n\n- `class hypothesis.database.InMemoryExampleDatabase[source]`  \n  A non-persistent example database, implemented in terms of an in-memory dictionary. This can be useful if you call a test function several times in a single session, or for testing other database implementations, but because it does not persist between runs we do not recommend it for general use.\n\n- `class hypothesis.database.DirectoryBasedExampleDatabase(path)[source]`  \n  Use a directory to store Hypothesis examples as files. Each test corresponds to a directory, and each example to a file within that directory. While the contents are fairly opaque, a `DirectoryBasedExampleDatabase` can be shared by checking the directory into version control, for example with the following `.gitignore`:  \n  ```gitignore\n  # Ignore files cached by Hypothesis...\n  .hypothesis/*\n  # except for the examples directory\n  !.hypothesis/examples/\n  ```  \n  Note however that this only makes sense if you also pin to an exact version of Hypothesis, and we would usually recommend implementing a shared database with a network datastore — see `ExampleDatabase`, and the `MultiplexedDatabase` helper.\n\n- `class hypothesis.database.GitHubArtifactDatabase(owner, repo, artifact_name='hypothesis-example-db', cache_timeout=datetime.timedelta(days=1), path=None)`  \n  A file-based database loaded from a GitHub Actions artifact. You can use this for sharing example databases between CI runs and developers, allowing the latter to get read-only access to the former. This is particularly useful for continuous fuzzing (i.e. with HypoFuzz), where the CI system can help find new failing examples through fuz"
  },
  {
    "book_name": "api.html",
    "abstract": "(part 1/2) 详细说明了`GitHubArtifactDatabase`的使用方法，包括所需的`GITHUB_TOKEN`环境变量、与`MultiplexedDatabase`和`ReadOnlyDatabase`结合使用的典型配置，以及配套的GitHub Actions工作流示例。",
    "content": "zing, and developers can reproduce them locally without any manual effort.  \n\n  > **Note**  \n  > You must provide `GITHUB_TOKEN` as an environment variable. In CI, Github Actions provides this automatically, but it needs to be set manually for local usage. In a developer machine, this would usually be a Personal Access Token. If the repository is private, it’s necessary for the token to have `repo` scope in the case of a classic token, or `actions:read` in the case of a fine-grained token.  \n\n  In most cases, this will be used through the `MultiplexedDatabase`, by combining a local directory-based database with this one. For example:  \n  ```python\n  local = DirectoryBasedExampleDatabase(\".hypothesis/examples\")\n  shared = ReadOnlyDatabase(GitHubArtifactDatabase(\"user\", \"repo\"))\n  settings.register_profile(\"ci\", database=local)\n  settings.register_profile(\"dev\", database=MultiplexedDatabase(local, shared))\n  # We don't want to use the shared database in CI, only to populate its local one.\n  # which the workflow should then upload as an artifact.\n  settings.load_profile(\"ci\" if os.environ.get(\"CI\") else \"dev\")\n  ```  \n\n  > **Note**  \n  > Because this database is read-only, you always need to wrap it with the `ReadOnlyDatabase`.  \n\n  A setup like this can be paired with a GitHub Actions workflow including something like the following:  \n  ```yaml\n  - name: Download example database\n    uses: dawidd6/action-download-artifact@v9\n    with:\n      name: hypothesis-example-db\n      path: .hypothesis/examples\n      if_no_artifact_found: warn\n      workflow_conclusion: completed\n  - name: Run tests\n    run: pytest\n  - name: Upload example database\n    uses: actions/upload-artifact@v3\n    if: always()\n    with:\n      name: hypothesis-example-db\n      path: .hypothesis/examples\n  ```  \n  In this workflow, we use dawidd6/action-download-artifact to download the latest artifact given that the official actions/download-artifact does not support downloading artifacts from previous wo"
  },
  {
    "book_name": "api.html",
    "abstract": "(part 2/2) 介绍了用于组合和增强数据库功能的包装器类：`ReadOnlyDatabase`、`MultiplexedDatabase`和`BackgroundWriteDatabase`，并展示了如何将它们结合使用以实现本地与共享数据库的协同工作。",
    "content": "rkflow runs. The database automatically implements a simple file-based cache with a default expiration period of 1 day. You can adjust this through the `cache_timeout` property. For mono-repo support, you can provide a unique `artifact_name` (e.g. `hypofuzz-example-db-frontend`).\n\n- `class hypothesis.database.ReadOnlyDatabase(db)[source]`  \n  A wrapper to make the given database read-only. The implementation passes through `fetch`, and turns `save`, `delete`, and `move` into silent no-ops. Note that this disables Hypothesis’ automatic discarding of stale examples. It is designed to allow local machines to access a shared database (e.g. from CI servers), without propagating changes back from a local or in-development branch.\n\n- `class hypothesis.database.MultiplexedDatabase(*dbs)[source]`  \n  A wrapper around multiple databases. Each `save`, `fetch`, `move`, or `delete` operation will be run against all of the wrapped databases. `fetch` does not yield duplicate values, even if the same value is present in two or more of the wrapped databases. This combines well with a `ReadOnlyDatabase`, as follows:  \n  ```python\n  local = DirectoryBasedExampleDatabase(\"/tmp/hypothesis/examples/\")\n  shared = CustomNetworkDatabase()\n  settings.register_profile(\"ci\", database=shared)\n  settings.register_profile(\n      \"dev\", database=MultiplexedDatabase(local, ReadOnlyDatabase(shared))\n  )\n  settings.load_profile(\"ci\" if os.environ.get(\"CI\") else \"dev\")\n  ```So your CI system or fuzzing runs can populate a central shared database; while local runs on development machines can reproduce any failures from CI but will only cache their own failures locally and cannot remove examples from the shared database.\n\n- class hypothesis.database.BackgroundWriteDatabase(db)[source]¶  \n  A wrapper which defers writes on the given database to a background thread.  \n  Calls to `fetch()` wait for any enqueued writes to finish before fetching from the database.\n\n- class hypothesis.extra.redis.RedisExample"
  },
  {
    "book_name": "api.html",
    "abstract": "(part 1/2) 介绍了`RedisExampleDatabase`用于在Redis中存储示例，并开始讲解状态测试（Stateful tests），包括`RuleBasedStateMachine`类和`rule`装饰器，后者用于定义状态机的规则及其输入输出。",
    "content": "Database(redis, *, expire_after=datetime.timedelta(days=8), key_prefix=b'hypothesis-example:', listener_channel='hypothesis-changes')  \n  Store Hypothesis examples as sets in the given Redis datastore. This is particularly useful for shared databases, as per the recipe for a `MultiplexedDatabase`.\n\n  **Note**  \n  If a test has not been run for `expire_after`, those examples will be allowed to expire. The default time-to-live persists examples between weekly runs.\n\n## Stateful tests\n\n- class hypothesis.stateful.RuleBasedStateMachine[source]¶  \n  A RuleBasedStateMachine gives you a structured way to define state machines.  \n  The idea is that a state machine carries the system under test and some supporting data. This data can be stored in instance variables or divided into Bundles. The state machine has a set of rules which may read data from bundles (or just from normal strategies), push data onto bundles, change the state of the machine, or verify properties. At any given point a random applicable rule will be executed.\n\n## Rules\n\n- hypothesis.stateful.rule(*, targets=(), target=None, **kwargs)[source]¶  \n  Decorator for RuleBasedStateMachine. Any Bundle present in `target` or `targets` will define where the end result of this function should go. If both are empty then the end result will be discarded. `target` must be a Bundle, or if the result should be replicated to multiple bundles you can pass a tuple of them as the `targets` argument. It is invalid to use both arguments for a single rule. If the result should go to exactly one of several bundles, define a separate rule for each case.  \n  `kwargs` then define the arguments that will be passed to the function invocation. If their value is a Bundle, or if it is `consumes(b)` where `b` is a Bundle, then values that have previously been produced for that bundle will be provided. If `consumes` is used, the value will also be removed from the bundle. Any other kwargs should be strategies and values from them will be"
  },
  {
    "book_name": "api.html",
    "abstract": "(part 2/2) 详细介绍了状态测试中用于操作Bundle的辅助函数`consumes`和`multiple`，以及`Bundle`类本身的定义和用法，并引入了`initialize`装饰器，用于在规则执行前进行初始化。",
    "content": " provided.\n\n- hypothesis.stateful.consumes(bundle)[source]¶  \n  When introducing a rule in a RuleBasedStateMachine, this function can be used to mark bundles from which each value used in a step with the given rule should be removed. This function returns a strategy object that can be manipulated and combined like any other.  \n  For example, a rule declared with  \n  `@rule(value1=b1, value2=consumes(b2), value3=lists(consumes(b3)))`  \n  will consume a value from Bundle `b2` and several values from Bundle `b3` to populate `value2` and `value3` each time it is executed.\n\n- hypothesis.stateful.multiple(*args)[source]¶  \n  This function can be used to pass multiple results to the target(s) of a rule. Just use `return multiple(result1, result2, ...)` in your rule.  \n  It is also possible to use `return multiple()` with no arguments in order to end a rule without passing any result.\n\n- class hypothesis.stateful.Bundle(name, *, consume=False, draw_references=True)[source]¶  \n  A collection of values for use in stateful testing.  \n  Bundles are a kind of strategy where values can be added by rules, and (like any strategy) used as inputs to future rules.  \n  The `name` argument they are passed is the name they are referred to internally by the state machine; no two bundles may have the same name. It is idiomatic to use the attribute being assigned to as the name of the Bundle:  \n  ```python\n  class MyStateMachine(RuleBasedStateMachine):\n      keys = Bundle(\"keys\")\n  ```  \n  Bundles can contain the same value more than once; this becomes relevant when using `consumes()` to remove values again.  \n  If the `consume` argument is set to True, then all values that are drawn from this bundle will be consumed (as above) when requested.\n\n- hypothesis.stateful.initialize(*, targets=(), target=None, **kwargs)[source]¶  \n  Decorator for RuleBasedStateMachine.  \n  An initialize decorator behaves like a rule, but all `@initialize()` decorated methods will be called before any `@rule()` de"
  },
  {
    "book_name": "api.html",
    "abstract": "(part 1/2) 介绍了`precondition`装饰器用于为规则设置前置条件，`invariant`装饰器用于定义状态机不变量，并说明了如何通过`run_state_machine_as_test()`手动运行状态机测试。",
    "content": "corated methods, in an arbitrary order. Each `@initialize()` method will be called exactly once per run, unless one raises an exception – after which only the `.teardown()` method will be run. `@initialize()` methods may not have preconditions.\n\n- hypothesis.stateful.precondition(precond)[source]¶  \n  Decorator to apply a precondition for rules in a RuleBasedStateMachine. Specifies a precondition for a rule to be considered as a valid step in the state machine, which is more efficient than using `assume()` within the rule. The `precond` function will be called with the instance of RuleBasedStateMachine and should return True or False. Usually it will need to look at attributes on that instance.  \n  For example:\n  ```python\n  class MyTestMachine(RuleBasedStateMachine):\n      state = 1\n\n      @precondition(lambda self: self.state != 0)\n      @rule(numerator=integers())\n      def divide_with(self, numerator):\n          self.state = numerator / self.state\n  ```  \n  If multiple preconditions are applied to a single rule, it is only considered a valid step when all of them return True. Preconditions may be applied to invariants as well as rules.\n\n- hypothesis.stateful.invariant(*, check_during_init=False)[source]¶  \n  Decorator to apply an invariant for rules in a RuleBasedStateMachine. The decorated function will be run after every rule and can raise an exception to indicate failed invariants.  \n  For example:\n  ```python\n  class MyTestMachine(RuleBasedStateMachine):\n      state = 1\n\n      @invariant()\n      def is_nonzero(self):\n          assert self.state != 0\n  ```  \n  By default, invariants are only checked after all `@initialize()` rules have been run. Pass `check_during_init=True` for invariants which can also be checked during initialization.\n\n## Running state machines\n\nIf you want to bypass the TestCase infrastructure you can invoke these manually. The stateful module exposes `run_state_machine_as_test()`, which takes an arbitrary function returning a `RuleBasedS"
  },
  {
    "book_name": "api.html",
    "abstract": "(part 2/2) 详细说明了`run_state_machine_as_test`函数的用法，并列举了Hypothesis自定义的异常类，包括通用异常、弃用警告以及用于处理非确定性失败（Flaky）的各类异常。",
    "content": "tateMachine` and an optional settings parameter and does the same as the class based `runTest` provided.\n\n- hypothesis.stateful.run_state_machine_as_test(state_machine_factory, *, settings=None, _min_steps=0)  \n  Run a state machine definition as a test, either silently doing nothing or printing a minimal breaking program and raising an exception.  \n  `state_machine_factory` is anything which returns an instance of `RuleBasedStateMachine` when called with no arguments – it can be a class or a function. `settings` will be used to control the execution of the test.\n\n## Hypothesis exceptions\n\nCustom exceptions raised by Hypothesis.\n\n- class hypothesis.errors.HypothesisException[source]¶  \n  Generic parent class for exceptions thrown by Hypothesis.\n\n- class hypothesis.errors.HypothesisDeprecationWarning[source]¶  \n  A deprecation warning issued by Hypothesis.  \n  Actually inherits from FutureWarning, because DeprecationWarning is hidden by the default warnings filter.  \n  You can configure the `warnings` module to handle these warnings differently to others, either turning them into errors or suppressing them entirely. Obviously we would prefer the former!\n\n- class hypothesis.errors.Flaky[source]¶  \n  Base class for indeterministic failures. Usually one of the more specific subclasses (`FlakyFailure` or `FlakyStrategyDefinition`) is raised.  \n  See also the flaky failures tutorial.\n\n- class hypothesis.errors.FlakyStrategyDefinition[source]¶  \n  This function appears to cause inconsistent data generation.  \n  **Common causes for this problem are:**  \n  - The strategy depends on external state. e.g. it uses an external random number generator. Try to make a version that passes all the relevant state in from Hypothesis.  \n  See also the flaky failures tutorial.\n\n- class hypothesis.errors.FlakyFailure(msg, group)[source]¶  \n  This function appears to fail non-deterministically: We have seen it fail when passed this example at least once, but a subsequent invocation did not"
  },
  {
    "book_name": "api.html",
    "abstract": "(part 1/2) 继续介绍Hypothesis的异常类，详细说明了`FlakyFailure`和`FlakyBackendFailure`的触发原因，并列出了`InvalidArgument`、`ResolutionFailed`和`Unsatisfiable`等用于指示参数错误或策略无法满足的异常。",
    "content": " fail, or caused a distinct error.  \n  **Common causes for this problem are:**  \n  - The function depends on external state. e.g. it uses an external random number generator. Try to make a version that passes all the relevant state in from Hypothesis.  \n  - The function is suffering from too much recursion and its failure depends sensitively on where it’s been called from.  \n  - The function is timing sensitive and can fail or pass depending on how long it takes. Try breaking it up into smaller functions which don’t do that and testing those instead.  \n  See also the flaky failures tutorial.\n\n- class hypothesis.errors.FlakyBackendFailure(msg, group)[source]¶  \n  A failure was reported by an alternative backend, but this failure did not reproduce when replayed under the Hypothesis backend.  \n  When an alternative backend reports a failure, Hypothesis first replays it under the standard Hypothesis backend to check for flakiness. If the failure does not reproduce, Hypothesis raises this exception.\n\n- class hypothesis.errors.InvalidArgument[source]¶  \n  Used to indicate that the arguments to a Hypothesis function were in some manner incorrect.\n\n- class hypothesis.errors.ResolutionFailed[source]¶  \n  Hypothesis had to resolve a type to a strategy, but this failed.  \n  Type inference is best-effort, so this only happens when an annotation exists but could not be resolved for a required argument to the target of `builds()`, or where the user passed...\n\n- class hypothesis.errors.Unsatisfiable[source]¶  \n  We ran out of time or examples before we could find enough examples which sati"
  },
  {
    "book_name": "api.html",
    "abstract": "(part 2/2) 介绍了`DeadlineExceeded`异常，并开始讲解Hypothesis对Django测试的支持，包括如何通过继承特定的TestCase类来使用`@given`，以及关于避免使用`TransactionTestCase`的性能建议。",
    "content": "sfy the assumptions of this hypothesis.This could be because the function is too slow. If so, try upping the timeout. It could also be because the function is using `assume` in a way that is too hard to satisfy. If so, try writing a custom strategy or using a better starting point (e.g. if you are requiring a list has unique values you could instead filter out all duplicate values from the list).\n\n- class hypothesis.errors.DeadlineExceeded(runtime, deadline)[source]¶  \n  Raised when an input takes too long to run, relative to the `settings.deadline` setting.\n\n## Django\n\nSee also  \n[See the Django strategies reference](hypothesis.extra.django) for documentation on strategies in the `hypothesis.extra.django` module.\n\nHypothesis offers a number of features specific for Django testing, available in the `hypothesis[django]` extra. This is tested against each supported series with mainstream or extended support—if you’re still getting security patches, you can test with Hypothesis.\n\nUsing it is quite straightforward: All you need to do is subclass `hypothesis.extra.django.TestCase`, `hypothesis.extra.django.SimpleTestCase`, `hypothesis.extra.django.TransactionTestCase`, `LiveServerTestCase`, or `StaticLiveServerTestCase`, and you can use `@given` as normal, and the transactions will be per example rather than per test function as they would be if you used `@given` with a normal Django test suite (this is important because your test function will be called multiple times and you don’t want them to interfere with each other). Test cases on these classes that do not use `@given` will be run as normal for `django.test.TestCase` or `django.test.TransactionTestCase`.\n\nWe recommend avoiding `TransactionTestCase` unless you really have to run each test case in a database transaction. Because Hypothesis runs this in a loop, the performance problems `django.test.TransactionTestCase` normally has are significantly exacerbated and your tests will be really slow. If you are using `Tra"
  },
  {
    "book_name": "api.html",
    "abstract": "(part 1/2) 通过示例展示了如何使用`from_model()`为Django模型生成测试数据，并说明了如何通过`register_field_strategy`为自定义Django字段类型注册默认策略。",
    "content": "nsactionTestCase`, you may need to use `@settings(suppress_health_check=[HealthCheck.too_slow])` to avoid a `HealthCheck` error due to slow example generation.\n\nHaving set up a test class, you can now pass `@given` a strategy for Django models with `from_model()`.\n\nFor example, using the trivial django project we have for testing:\n```python\n>>> from hypothesis.extra.django import from_model\n>>> from toystore.models import Customer\n>>> c = from_model(Customer).example()\n>>> c\n<Customer: Customer object>\n>>> c.email\n'jaime.urbina@gmail.com'\n>>> c.name\n'\\U00109d3d\\U000e07be\\U000165f8\\U0003fabf\\U000c12cd\\U000f1910\\U00059f12\\U000519b0\\U0003fabf\\U000f1910\\U000423fb\\U000423fb\\U00059f12\\U000e07be\\U000c12cd\\U000e07be\\U000519b0\\U000165f8\\U0003fabf\\U0007bc31'\n>>> c.age\n-873375803\n```\nHypothesis has just created this with whatever the relevant type of data is. Obviously the customer’s age is implausible, which is only possible because we have not used (e.g.) `MinValueValidator` to set the valid range for this field (or used a `PositiveSmallIntegerField`, which would only need a maximum value validator).\n\nIf you do have validators attached, Hypothesis will only generate examples that pass validation. Sometimes that will mean that we fail a `HealthCheck` because of the filtering, so let’s explicitly pass a strategy to skip validation at the strategy level:\n\n## Custom field types\n\nIf you have a custom Django field type you can register it with Hypothesis’s model deriving functionality by registering a default strategy for it:\n```python\n>>> from toystore.models import CustomishField, Customish\n>>> from_model(Customish).example()\nhypothesis.errors.InvalidArgument: Missing arguments for mandatory field customish for model Customish\n>>> from hypothesis.extra.django import register_field_strategy\n>>> from hypothesis.strategies import just\n>>> register_field_strategy(CustomishField, just(\"hi\"))\n>>> x = from_model(Customish).example()\n>>> x.customish\n'hi'\n```\nNote that this mapping is on"
  },
  {
    "book_name": "api.html",
    "abstract": "(part 2/2) 介绍了如何使用`flatmap()`生成具有父子关系的Django模型，以及Hypothesis处理自定义主键和`MultiValueField`的方式。",
    "content": " exact type. Subtypes will not inherit it.\n\n## Generating child models\n\nFor the moment there’s no explicit support in hypothesis-django for generating dependent models. i.e. a Company model will generate no Shops. However if you want to generate some dependent models as well, you can emulate this by using the `.flatmap()` function as follows:\n\nLet’s unpack what this is doing:  \nThe way `flatmap` works is that we draw a value from the original strategy, then apply a function to it which gives us a new strategy. We then draw a value from that strategy. So in this case we’re first drawing a company, and then we’re drawing a list of shops belonging to that company: The `just()` strategy is a strategy such that drawing it always produces the individual value, so `from_model(Shop, company=just(company))` is a strategy that generates a Shop belonging to the original company.\n\nSo the following code would give us a list of shops all belonging to the same company:\n\nThe only difference from this and the above is that we want the company, not the shops. This is where the inner map comes in. We build the list of shops and then throw it away, instead returning the company we started for. This works because the models that Hypothesis generates are saved in the database, so we’re essentially running the inner strategy purely for the side effect of creating those children in the database.\n\n## Generating primary key values\n\nIf your model includes a custom primary key that you want to generate using a strategy (rather than a default auto-increment primary key) then Hypothesis has to deal with the possibility of a duplicate primary key.\n\nIf a model strategy generates a value for the primary key field, Hypothesis will create the model instance with `update_or_create()`, overwriting any existing instance in the database for this test case with the same primary key.\n\n## On the subject of MultiValueField\n\nDjango forms feature the `MultiValueField` which allows for several fields to be comb"
  },
  {
    "book_name": "api.html",
    "abstract": "(part 1/2) 解释了Django的`MultiValueField`在表单数据中的特殊处理方式，并介绍了`fuzz_one_input`属性，它允许将Hypothesis测试用作外部模糊器（fuzzer）的目标。",
    "content": "ined under a single named field, the default example of this is the `SplitDateTimeField`.\n\n```python\nclass CustomerForm(forms.Form):\n    name = forms.CharField()\n    birth_date_time = forms.SplitDateTimeField()\n```\n\n`from_form()` supports `MultiValueField` subclasses directly, however if you want to define your own strategy be forewarned that Django binds data for a `MultiValueField` in a peculiar way. Specifically each sub-field is expected to have its own entry in `data` addressed by the field name (e.g. `birth_date_time`) and the index of the sub-field within the `MultiValueField`, so form data for the example above might look like this:\n```python\n{\n    \"name\": \"Samuel John\",\n    \"birth_date_time_0\": \"2018-05-19\",  # the date, as the first sub-field\n    \"birth_date_time_1\": \"15:18:00\",   # the time, as the second sub-field\n}\n```\nThus, if you want to define your own strategies for such a field you must address your sub-fields appropriately:\n```python\nfrom_form(CustomerForm, birth_date_time_0=just(\"2018-05-19\"))\n```\n\n## External fuzzers\n\n- hypothesis.core.HypothesisHandle.fuzz_one_input = <property object>¶  \n  Run the test as a fuzz target, driven with the `buffer` of bytes.\n\nDepending on the passed `buffer` one of three things will happen:\n- If the bytestring was invalid, for example because it was too short or was filtered out by `assume()` or `.filter()`, `fuzz_one_input()` returns `None`.\n- If the bytestring was valid and the test passed, `fuzz_one_input()` returns a canonicalised and pruned bytestring which will replay that test case. This is provided as an option to improve the performance of mutating fuzzers, but can safely be ignored.\n- If the test failed, i.e. raised an exception, `fuzz_one_input()` will add the pruned buffer to the Hypothesis example database and then re-raise that exception. All you need to do to reproduce, minimize, and de-duplicate all the failures found via fuzzing is run your test suite!\n\nTo reduce the performance impact of database"
  },
  {
    "book_name": "api.html",
    "abstract": "(part 2/2) 详细说明了`fuzz_one_input`与`@settings`的交互，指出哪些设置项有效，并给出了使用示例和性能优化建议（如使用`BackgroundWriteDatabase`）。",
    "content": " writes, `fuzz_one_input()` only records failing inputs which would be valid shrinks for a known failure—meaning writes are somewhere between constant and log(N) rather than linear in runtime. However, this tracking only works within a persistent fuzzing process; for forkserver fuzzers we recommend `database=None` for the main run, and then replaying with a database enabled if you need to analyse failures.\n\nNote that the interpretation of both input and output bytestrings is specific to the exact version of Hypothesis you are using and the strategies given to the test, just like the database and `@reproduce_failure`.\n\n## Interaction with `@settings`\n\n`fuzz_one_input()` uses just enough of Hypothesis’ internals to drive your test function with a bytestring, and most settings therefore have no effect in this mode. We recommend running your tests the usual way before fuzzing to get the benefits of health checks, as well as afterwards to replay, shrink, deduplicate, and report whatever errors were discovered.\n\n- `settings.database` is used by `fuzz_one_input()`—adding failures to the database to be replayed when you next run your tests is our preferred reporting mechanism and response to the ‘fuzzer taming’ problem.\n- `settings.verbosity` and `settings.stateful_step_count` work as usual.\n- The `deadline`, `derandomize`, `max_examples`, `phases`, `print_blob`, `report_multiple_bugs`, and `suppress_health_check` settings do not affect `fuzz_one_input()`.\n\n## Example Usage\n\n```python\n@given(st.text())\ndef test_foo(s):\n    ...\n\n# This is a traditional fuzz target - call it with a bytestring,\n# or a binary IO object, and it runs the test once.\nfuzz_target = test_foo.hypothesis.fuzz_one_input\n\n# For example:\nfuzz_target(b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\")\nfuzz_target(io.BytesIO(b\"\\x01\"))\n```\n\n**Tip**  \nIf you expect to discover many failures while using `fuzz_one_input()`, consider wrapping your database with `BackgroundWriteDatabase`, for low-overhead writes of failures.\n\n*"
  },
  {
    "book_name": "api.html",
    "abstract": "(part 1/2) 介绍了Hypothesis的自定义函数执行（Custom function execution）功能，通过定义`execute_example`方法来控制测试示例的运行方式（如设置/清理、子进程等），并说明了执行器（executor）的基本概念和要求。",
    "content": "*Tip**  \nWant an integrated workflow for your team’s local tests, CI, and continuous fuzzing? Use [HypoFuzz](https://hypofuzz.com/) to fuzz your whole test suite, and find more bugs with the same tests!\n\nSee also  \n[See also the Use Hypothesis with an external fuzzer how-to.](https://hypothesis.readthedocs.io/en/latest/how-to-guides.html#use-hypothesis-with-an-external-fuzzer)\n\n## Custom function execution\n\nHypothesis provides you with a hook that lets you control how it runs examples.This lets you do things like set up and tear down around each example, run examples in a subprocess, transform coroutine tests into normal tests, etc. For example, `TransactionTestCase` in the Django extra runs each example in a separate database transaction.\n\nThe way this works is by introducing the concept of an executor. An executor is essentially a function that takes a block of code and runs it. The default executor is:\n\n```python\ndef default_executor(function):\n    return function()\n```\n\nYou define executors by defining a method `execute_example` on a class. Any test methods on that class with `@given` used on them will use `self.execute_example` as an executor with which to run tests. For example, the following executor runs all its code twice:\n\n> **Note:** The functions you use in `map`, etc. will run inside the executor. i.e. they will not be called until you invoke the function passed to `execute_example`.\n\nAn executor must be able to handle being passed a function which returns `None`, otherwise it won’t be able to run normal test cases. So for example the following executor is invalid:\n\n```python\nfrom unittest import TestCase\n\nclass TestRunTwice(TestCase):\n    def execute_example(self, f):\n        return f()()\n```\n\nand should be rewritten as:\n\nAn alternative hook is provided for use by test runner extensions such as `pytest-trio`, which cannot use the `execute_example` method. This is not recommended for end-users—it is better to write a complete test function directly, per"
  },
  {
    "book_name": "builtin-strategies.html",
    "abstract": "介绍了Hypothesis库提供的一系列内置策略，包括用于生成整数、浮点数、布尔值、文本、列表、元组等的策略，并说明了如何组合和使用它们。",
    "content": "# Built-in strategies\n\nThis page shows some of the strategies that Hypothesis provides for you.\n\n## Strategies provided by Hypothesis\n\nHere is a selection of strategies provided by Hypothesis that may be useful to know:\n\n- `integers()`  \n  Generates integers.\n\n- `floats()`  \n  Generates floats.\n\n- `booleans()`  \n  Generates booleans.\n\n- `text()`  \n  Generates unicode strings (i.e., instances of `str`). Can be constrained to ASCII with `st.text(st.characters(codec=\"ascii\"))`.\n\n- `.lists()`  \n  Generates lists with elements from the strategy passed to it.  \n  `st.lists(st.integers())` generates lists of integers.\n\n- `tuples()`  \n  Generates tuples of a fixed length.  \n  `st.tuples(st.integers(), st.floats())` generates tuples with two elements, where the first element is an integer and the second is a float.\n\n- `one_of()`  \n  Generates from any of the strategies passed to it.  \n  `st.one_of(st.integers(), st.floats())` generates either integers or floats. You can also use `|` to construct `one_of()`, like `st.integers() | st.floats()`.\n\n- `builds()`  \n  Generates instances of a class (or other callable) by specifying a strategy for each argument, like `st.builds(Person, name=st.text(), age=st.integers())`.\n\n- `just()`  \n  Generates the exact value passed to it.  \n  `st.just(\"a\")` generates the exact string `\"a\"`. This is useful when something expects to be passed a strategy. For instance, `st.lists(st.integers() | st.just(\"a\"))` generates lists whose elements are either integers or the string `\"a\"`.\n\n- `sampled_from()`  \n  Generates a random value from a list.  \n  `st.sampled_from([\"a\", 1])` is roughly equivalent to `st.just(\"a\") | st.just(1)`.\n\n- `none()`  \n  Generates `None`. Useful for parameters that can be optional, like `st.integers() | st.none()`.\n\nSee also: [See the strategies API reference](#) for a full list of strategies provided by Hypothesis."
  },
  {
    "book_name": "domain.html",
    "abstract": "Hypothesis区分策略的“域”（可能生成的输入集合）和“分布”（各输入的生成概率），并主张由库而非用户控制分布。建议用户为测试选择最通用的策略域，以覆盖所有潜在的边界情况，仅在必要时因性能原因限制输入规模。",
    "content": "Domain and distribution\n\nNote\n\nThis page is primarily for users who may be familiar with other property-based testing libraries, and who expect control over the distribution of inputs in Hypothesis, via e.g. a `scale` parameter for size or a `frequency` parameter for relative probabilities.\n\nHypothesis makes a distinction between the domain of a strategy, and the distribution of a strategy.\n\nThe domain is the set of inputs that should be possible to generate. For instance, in `lists(integers())`, the domain is lists of integers. For other strategies, particularly those that use `@composite` or `assume()` in their definition, the domain might be more complex.\n\nThe distribution is the probability with which different elements in the domain should be generated. For `lists(integers())`, should Hypothesis generate many small lists? Large lists? More positive or more negative numbers? etc.\n\nHypothesis takes a philosophical stance that while users may be responsible for selecting the domain, the property-based testing library—not the user—should be responsible for selecting the distribution. As an intentional design choice, Hypothesis therefore lets you control the domain of inputs to your test, but not the distribution.\n\nHow should I choose a domain for my test?\n\nWe recommend using the most-general strategy for your test, so that it can in principle generate any edge case for which the test should pass. Limiting the size of generated inputs, and especially limiting the variety of inputs, can all too easily exclude the bug-triggering values from consideration - and be the difference between a test which finds the bug, or fails to do so.\n\nSometimes size limits are necessary for performance reasons, but we recommend limiting your strategies only after you’ve seen substantial slowdowns without limits. Far better to find bugs slowly, than not find them at all - and you can manage performance with the `phases` or `max_examples` settings rather than weakening the test."
  },
  {
    "book_name": "domain.html",
    "abstract": "Hypothesis不开放分布控制权，原因包括：人类不擅长选择能发现未知bug的分布；理想分布依赖于具体代码和测试属性；分布是内部实现细节，公开会阻碍库的改进；且该功能更适合由替代后端（如hypofuzz）处理。",
    "content": "\n\nWhy not let users control the distribution?\n\nThere are a few reasons Hypothesis doesn’t let users control the distribution.\n\n- Humans are pretty bad at choosing bug-finding distributions! Some bugs are “known unknowns”: you suspected that a part of the codebase was buggy in a particular way. Others are “unknown unknowns”: you didn’t know that a bug was possible until stumbling across it. Humans tend to overtune distributions for the former kind of bug, and not enough for the latter.\n- The ideal strategy distribution depends not only on the codebase, but also on the property being tested. A strategy used in many places may have a good distribution for one property, but not another.\n- The distribution of inputs is a deeply internal implementation detail. We sometimes change strategy distributions, either explicitly, or implicitly from other work on the Hypothesis engine. Exposing this would lock us into a public API that may make improvements to Hypothesis more difficult.\n- Finally, we think distribution control is better handled with alternative backends. If existing backends like `hypofuzz` and `crosshair` don’t suit your needs, you can also write your own. Backends can automatically generalize and adapt to the strategy and property being tested and avoid most of the problems above."
  },
  {
    "book_name": "domain.html",
    "abstract": "Hypothesis的默认分布旨在高效发现bug，而非均匀或“真实”。其具体实现是复杂的，并结合了多种静态策略设计、动态引擎特性和对被测代码的分析。该分布是活跃的研究领域，会持续改进。",
    "content": "\n\nWe’re not saying that control over the distribution isn’t useful! We occasionally receive requests to expose the distribution in Hypothesis (e.g.), and users wouldn’t be asking for it if it wasn’t. However, adding this to the public strategy API would make it easy for users to unknowingly weaken their tests, and would add maintenance overhead to Hypothesis, and so we haven’t yet done so.\n\nOkay, but what is the distribution?\n\nAn exact answer depends on both the strategy or strategies for the tests, and the code being tested - but we can make some general remarks, starting with “it’s actually really complicated”.\n\nHypothesis’ default configuration uses a distribution which is tuned to maximize the chance of finding bugs, in as few executions as possible. We explicitly don’t aim for a uniform distribution, nor for a ‘realistic’ distribution of inputs; Hypothesis’ goal is to search the domain for a failing input as efficiently as possible.\n\nThe test case distribution remains an active area of research and development, and we change it whenever we think that would be a net improvement for users. Today, Hypothesis’ default distribution is shaped by a wide variety of techniques and heuristics:\n\n- Some are statically designed into strategies - for example, `integers()` upweights range endpoints, and samples from a mixed distribution over integer bit-widths.\n- Some are dynamic features of the engine - like replaying prior examples with subsections of the input ‘cloned’ or otherwise altered, for bugs which trigger only when different fields have the same value (which is otherwise exponentially unlikely).\n- Some vary depending on the code under test - we collect interesting-looking constants from imported source files as seeds for test cases.\n\nAnd as if that wasn’t enough, alternative backends can radically change the distribution again - for example `hypofuzz` uses runtime feedbac"
  }
]